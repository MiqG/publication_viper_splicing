import os
import pandas as pd

# variables
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
RAW_DIR = os.path.join(ROOT,"data","raw")
SRC_DIR = os.path.join(ROOT,"src")
PREP_DIR = os.path.join(ROOT,"data","prep")
SUPPORT_DIR = os.path.join(ROOT,"support")
RESULTS_DIR = os.path.join(ROOT,"results","preprocess_data")

SAVE_PARAMS = {"sep":"\t", "index":False, "compression":"gzip"}

EVENT_TYPES = ["EX"]
CELL_LINES = ["K562","HepG2"]
DPSI_TYPES = ["delta_psi"]
CANCER_TYPES = [
    'ACC',
    'BLCA',
    'BRCA',
    'CESC',
    'CHOL',
    'COAD',
    'DLBC',
    'ESCA',
    'GBM',
    'HNSC',
    'KICH',
    'KIRC',
    'KIRP',
    'LAML',
    'LGG',
    'LIHC',
    'LUAD',
    'LUSC',
    'MESO',
    'OV',
    'PAAD',
    'PCPG',
    'PRAD',
    'READ',
    'SARC',
    'SKCM',
    'STAD',
    'TGCT',
    'THCA',
    'THYM',
    'UCEC',
    'UCS',
    'UVM'
]

DATASETS = ["CCLE"]

ENCORE_DATASETS = ["ENCOREKD","ENCOREKO"]

N_SAMPLES_ENA = {
    "sf_drugs": 108,
    "sf_ptms": 88,
    "ipsc_differentiation": 217 
}

tcga_metadata = pd.read_table(os.path.join(RAW_DIR,"TCGA","metadata","PANCAN.tsv.gz"))
TCGA_N_SAMPLES = tcga_metadata["cancer_type"].value_counts().to_dict()
tcga_metadata = tcga_metadata.groupby(
    ["sample_type_clean","cancer_type"]
).size().reset_index().rename(columns={0:"n"})
tcga_metadata = tcga_metadata.loc[tcga_metadata["n"]>=3]

CANCER_TYPES_SPLIT = tcga_metadata["cancer_type"].values
SAMPLE_TYPES_SPLIT = tcga_metadata["sample_type_clean"].values
CANCER_TYPES_STN = tcga_metadata.loc[tcga_metadata["sample_type_clean"]=="SolidTissueNormal","cancer_type"].values
CANCER_TYPES_PT = tcga_metadata.loc[tcga_metadata["sample_type_clean"]=="PrimaryTumor","cancer_type"].values
CANCER_TYPES_CONCAT = {"SolidTissueNormal": CANCER_TYPES_STN, "PrimaryTumor": CANCER_TYPES_PT}
        
##### RULES #####
rule all:
    input:
        # get list of splicing factors
        os.path.join(PREP_DIR,"references","splicing_factors"),
        
        # preprocess CCLE
        os.path.join(PREP_DIR,"metadata","CCLE.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CCLE-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CCLE-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CCLE-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CCLE-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","CCLE.tsv.gz"),
        os.path.join(PREP_DIR,"mutations","CCLE.tsv.gz"),

        # preprocess ENCORE
        os.path.join(PREP_DIR,"metadata","ENCOREKD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKD-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKD-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKD-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKD-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","ENCOREKD.tsv.gz"),    
        
        # preprocess ENCOREKO
        os.path.join(PREP_DIR,"metadata","ENCOREKO.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKO-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKO-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKO-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENCOREKO-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","ENCOREKO.tsv.gz"),    

        # preprocess TCGA
        expand(os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz"), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,"event_psi","{cancer}-EX.tsv.gz"), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,"event_psi","{cancer}-ALTA.tsv.gz"), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,"event_psi","{cancer}-ALTD.tsv.gz"), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,"event_psi","{cancer}-INT.tsv.gz"), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,'genexpr_tpm','{cancer}.tsv.gz'), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,'genexpr_counts','{cancer}.tsv.gz'), cancer=CANCER_TYPES),
        expand(os.path.join(PREP_DIR,"mutations","{cancer}.tsv.gz"), cancer=CANCER_TYPES),
        os.path.join(PREP_DIR,"metadata","PANCAN.tsv.gz"),
        
        # preprocess ENASFS
        os.path.join(PREP_DIR,"metadata","ENASFS.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENASFS-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENASFS-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENASFS-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","ENASFS-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","ENASFS.tsv.gz"),
        
        # PERTURBATION SCREENS
        ### ENCORE
        ### log FC TPM
        expand(os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}',"{cell_line}",'log2_fold_change_tpm.tsv.gz'), cell_line=CELL_LINES, dataset=ENCORE_DATASETS),
        ### simplify log FC TPM
        expand(os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'log2_fold_change_tpm.tsv.gz'), cell_line=CELL_LINES, dataset=ENCORE_DATASETS),
        
        ### delta PSI
        expand(os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}',"{cell_line}",'delta_psi-{event_type}.tsv.gz'), event_type=EVENT_TYPES, cell_line=CELL_LINES, dataset=ENCORE_DATASETS),
        ### simplify delta PSI
        expand(os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'{dpsi_type}-{event_type}.tsv.gz'), event_type=EVENT_TYPES, dpsi_type=DPSI_TYPES, cell_line=CELL_LINES, dataset=ENCORE_DATASETS),
        
        ## ENASFS
        ### genexpr fold change
        os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','log2_fold_change_tpm.tsv.gz'),
        ### simplify genexpr fold change
        os.path.join(PREP_DIR,'ground_truth_pert','ENASFS','log2_fold_change_tpm.tsv.gz'),
        ### delta PSI
        expand(os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','delta_psi-{event_type}.tsv.gz'), event_type=EVENT_TYPES),
        ### simplify delta PSI
        expand(os.path.join(PREP_DIR,'ground_truth_pert','ENASFS','delta_psi-{event_type}.tsv.gz'), event_type=EVENT_TYPES),
        
        # impute event PSI
        expand(os.path.join(PREP_DIR,'event_psi_imputed','{dataset}-{event_type}.tsv.gz'), dataset=["CardosoMoreira2020","PANCAN-SolidTissueNormal","PANCAN-PrimaryTumor"], event_type=EVENT_TYPES),
        # benchmark event imputation
        expand(os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}.tsv.gz'), dataset=["CardosoMoreira2020"], event_type=EVENT_TYPES),
        expand(os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}-k{k}.tsv.gz'), dataset=["CardosoMoreira2020"], event_type=EVENT_TYPES, k=["2","5","10","50","100"]),
        
        # compute summary stats
        expand(os.path.join(PREP_DIR,"summary_stats","event_psi_imputed","{dataset}-EX.tsv.gz"), dataset=DATASETS),
        expand(os.path.join(PREP_DIR,"summary_stats","genexpr_tpm","{dataset}.tsv.gz"), dataset=DATASETS),
        
        # preprocess DepMap
        os.path.join(PREP_DIR,"demeter2","CCLE.tsv.gz"),
        
        # preprocess Nijhuis2020
        os.path.join(PREP_DIR,"metadata","Nijhuis2020.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Nijhuis2020-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Nijhuis2020-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Nijhuis2020-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Nijhuis2020-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","Nijhuis2020.tsv.gz"),

        # preprocess Lu2021
        os.path.join(PREP_DIR,"metadata","Lu2021.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Lu2021-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Lu2021-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Lu2021-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Lu2021-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","Lu2021.tsv.gz"),
        
        # preprocess ENA datasets
        expand(os.path.join(PREP_DIR,"metadata","{dataset}.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        expand(os.path.join(PREP_DIR,"event_psi","{dataset}-EX.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        expand(os.path.join(PREP_DIR,"event_psi","{dataset}-ALTA.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        expand(os.path.join(PREP_DIR,"event_psi","{dataset}-ALTD.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        expand(os.path.join(PREP_DIR,"event_psi","{dataset}-INT.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        expand(os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"), dataset=N_SAMPLES_ENA.keys()),
        
        # preprocess CardosoMoreira2020
        os.path.join(PREP_DIR,"metadata","CardosoMoreira2020.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CardosoMoreira2020-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CardosoMoreira2020-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CardosoMoreira2020-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","CardosoMoreira2020-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","CardosoMoreira2020.tsv.gz"),
        
        # preprocess tumorigenesis
        os.path.join(PREP_DIR,"metadata","tumorigenesis.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","tumorigenesis-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","tumorigenesis-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","tumorigenesis-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","tumorigenesis-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","tumorigenesis.tsv.gz"),

        # preprocess Riaz2017
        os.path.join(PREP_DIR,"metadata","Riaz2017.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Riaz2017-EX.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Riaz2017-ALTA.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Riaz2017-ALTD.tsv.gz"),
        os.path.join(PREP_DIR,"event_psi","Riaz2017-INT.tsv.gz"),
        os.path.join(PREP_DIR,"genexpr_tpm","Riaz2017.tsv.gz"),
        
        # preprocess STRINGDB
        os.path.join(PREP_DIR,'ppi','STRINGDB.tsv.gz'),
        
        # TCGA for network inference
        ## split by cancer type and sample type
        expand(os.path.join(PREP_DIR,"event_psi","{cancer}-{sample}-EX.tsv.gz"), zip, cancer=CANCER_TYPES_SPLIT, sample=SAMPLE_TYPES_SPLIT),
        expand(os.path.join(PREP_DIR,"genexpr_tpm","{cancer}-{sample}.tsv.gz"), zip, cancer=CANCER_TYPES_SPLIT, sample=SAMPLE_TYPES_SPLIT),
        ## concatenate by sample type
        expand(os.path.join(PREP_DIR,"event_psi","PANCAN-{sample}-EX.tsv.gz"), sample=["SolidTissueNormal","PrimaryTumor"]),
        expand(os.path.join(PREP_DIR,"genexpr_tpm","PANCAN-{sample}.tsv.gz"), sample=["SolidTissueNormal","PrimaryTumor"]),
        
        # POSTAR3 preprocessing
        os.path.join(PREP_DIR,"clip_peaks_mapped","POSTAR3.tsv.gz"),
        
        # preprocess SplicingLore
        os.path.join(PREP_DIR,"metadata","SplicingLore.tsv.gz"),
        os.path.join(PREP_DIR,"ground_truth_pert","SplicingLore","delta_psi-EX.tsv.gz"),

        # EDA data figures
        os.path.join(RESULTS_DIR,'figures','eda'),
        
        # figures benchmark KNN imputation
        os.path.join(RESULTS_DIR,'figures','benchmark_imputation')


rule make_list_splicing_factors:
    input:
        spliceosome_db = os.path.join(SUPPORT_DIR,"splicing_factors","literature_suptabs","SpliceosomeDB-human.csv"),
        head = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors_Head2021-ensembl.txt"),
        rogalska = os.path.join(SUPPORT_DIR,"splicing_factors","literature_suptabs","Rogalska2022NatRevGen-suptab1.xlsx"),
        hegele = os.path.join(SUPPORT_DIR,"splicing_factors","literature_suptabs","Hegele2012MolCell-suptab1.xls"),
        seiler = os.path.join(SUPPORT_DIR,"splicing_factors","literature_suptabs","Seiler2018CellReports-suptab1.xlsx"),
        papasaikas = os.path.join(SUPPORT_DIR,"splicing_factors","literature_suptabs","Papasaikas2015MolCell-suptab2.xlsx"),
        go_rna_splicing = os.path.join(SUPPORT_DIR,"splicing_factors","GOBP_RNA_SPLICING-ensembl.txt"),
        go_rna_binding = os.path.join(SUPPORT_DIR,"splicing_factors","GOMF_RNA_BINDING-ensembl.txt"),
        annot_hgnc = os.path.join(RAW_DIR,"HGNC","gene_annotations.tsv.gz"),
        annot_ensembl = os.path.join(RAW_DIR,"references","annotation-entrez_ensembl_symbol.tsv")
    output:
        directory(os.path.join(PREP_DIR,"references","splicing_factors"))
    run:
        import pandas as pd
        import numpy as np
        
        # load data
        spliceosome_db = pd.read_csv(input.spliceosome_db)
        rogalska = pd.read_excel(input.rogalska)
        hegele = pd.read_excel(input.hegele)
        seiler = pd.read_excel(input.seiler, skiprows=3, nrows=404)
        papasaikas = pd.read_excel(input.papasaikas)
        head = pd.read_table(input.head, header=None)[0].tolist()
        go_rna_splicing = pd.read_table(input.go_rna_splicing, header=None)[0].tolist()
        go_rna_binding = pd.read_table(input.go_rna_binding, header=None)[0].tolist()
        annot_hgnc = pd.read_table(input.annot_hgnc)
        
        # prepare symbol cleanup
        cleanup = annot_hgnc[["Approved symbol","Ensembl gene ID","NCBI Gene ID","Alias symbols","Previous symbols"]].copy()
        cleanup["aliases"] = cleanup["Alias symbols"] + ", " + cleanup["Previous symbols"]
        cleanup["aliases"] = cleanup["aliases"].str.split(", ")
        cleanup = cleanup.explode("aliases")
        
        annot = annot_hgnc[["Approved symbol","Ensembl gene ID","NCBI Gene ID"]].drop_duplicates().copy()
        annot = annot.rename(columns={
            "Approved symbol":"GENE", "Ensembl gene ID":"ENSEMBL", "NCBI Gene ID":"ENTREZ"
        })
        
        # add ensembl in all lists
        ## Rogalska
        ### Fix with HGNC
        X = pd.merge(rogalska, annot[["GENE"]], left_on="SYMBOL", right_on="GENE", how="left")
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], cleanup, left_on="SYMBOL", right_on="aliases", how="left")["Approved symbol"].values
        ### Fix manually
        X.loc[X["SYMBOL"]=="HNRPLL", "GENE"] = "HNRNPLL"
        ### standardize
        X = pd.merge(X[["SYMBOL","CLASS","FULL_NAME","GENE"]], annot, on="GENE", how="left")
        ### save
        rogalska = X.copy()
        rogalska["dataset"] = "Rogalska2022"
        
        ## Hegele
        ### Fix with HGNC
        X = pd.merge(hegele, annot[["GENE"]], left_on="Symbol", right_on="GENE", how="left")
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], cleanup, left_on="Symbol", right_on="aliases", how="left")["Approved symbol"].values
        ### Fix manually
        X.loc[X["Symbol"]=="SF3B14", "GENE"] = "SF3B6"
        ### standardize
        cols_oi = ["Symbol","Category","Group_short"] + ["GENE"]
        X = pd.merge(X[cols_oi], annot, on="GENE", how="left")
        ### save
        hegele = X.copy()
        hegele["dataset"] = "Hegele2012"
        
        ## Seiler
        ### Fix with HGNC
        X = pd.merge(seiler, annot[["GENE"]], left_on="Gene Symbol", right_on="GENE", how="left")
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], cleanup, left_on="Gene Symbol", right_on="aliases", how="left")["Approved symbol"].values
        ### drop not found (LOC649330)
        idx_notfound = X["GENE"].isnull()
        X = X.loc[~idx_notfound]
        ### standardize
        cols_oi = ["GENE"]
        X = pd.merge(X[cols_oi], annot, on="GENE", how="left")
        ### save
        seiler = X.copy()
        seiler["dataset"] = "Seiler2018"
        
        ## Papasaikas
        ### drop experimental columns
        papasaikas = papasaikas.drop(columns=["SiRNA","Well","Pool Catalog Number","Duplex Catalog Number","Robin Reed classification"]).drop_duplicates().copy()
        papasaikas = papasaikas.loc[~(papasaikas["GENE ID"]=="-")].copy()
        ### Fix with HGNC
        X = pd.merge(papasaikas, annot[["GENE"]], left_on="Gene Symbol", right_on="GENE", how="left")
        X["GENE ID"] = X["GENE ID"].astype(float)
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], annot, left_on=["GENE ID"], right_on=["ENTREZ"], how="left")["GENE_y"].values
        ### standardize
        cols_oi = ['Gene Symbol', 'GENE ID', 'Luhrmann classification', 'Network Classification        (1st level )', 'Network Classification        (2nd level )'] + ["GENE"]
        X = pd.merge(X[cols_oi], annot, on="GENE", how="left")
        ### save
        papasaikas = X.copy()
        papasaikas["dataset"] = "Papasaikas2015"
        
        ## SpliceosomeDB
        X = spliceosome_db.copy()
        ### Fix manually
        X.loc[X["Gene Symbol"]=="C11orf2", "Gene Symbol"] = "VPS51"
        X = X.loc[~X["Ensemble Gene ID"].isnull()]
        ### Fix with HGNC
        X = pd.merge(X, annot[["GENE"]], left_on="Gene Symbol", right_on="GENE", how="left")
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], cleanup, left_on="Gene Symbol", right_on="aliases", how="left")["Approved symbol"].values
        ### Fix with HGNC - ENSEMBL
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound,"GENE"] = pd.merge(X.loc[idx_notfound], annot[["GENE","ENSEMBL"]], left_on="Ensemble Gene ID", right_on="ENSEMBL", how="left")["GENE_y"].values
        ### Fix manually
        idx_notfound = X["GENE"].isnull()
        X.loc[idx_notfound]
        X.loc[X["Gene Symbol"]=="HIST1H4A", "GENE"] = "H4C1"
        X.loc[X["Gene Symbol"]=="HIST2H4A", "GENE"] = "H4C14"
        ### standardize
        cols_oi = ['Gene Symbol', 'Complex', 'Ensemble Gene ID', 'Class / family'] + ["GENE"]
        X = pd.merge(X[cols_oi], annot, on="GENE", how="left")
        ### save
        spliceosome_db = X.copy()
        spliceosome_db["dataset"] = "SpliceosomeDB"
        
        ## Head2021
        head = annot.loc[annot["ENSEMBL"].isin(head)].copy()
        head["dataset"] = "Head2021"
        
        # combine datasets
        cols_oi = ["GENE","ENSEMBL","ENTREZ","dataset"]
        sfs_combined = pd.concat([
            rogalska[cols_oi],
            hegele[cols_oi],
            seiler[cols_oi],
            papasaikas[cols_oi],
            head[cols_oi]
        ])
        
        # select splicing factors
        splicing_factors = sfs_combined[["GENE","ENSEMBL","ENTREZ"]].drop_duplicates()
        
        # fix missing values
        splicing_factors.loc[splicing_factors["GENE"]=="RNU2-1","ENSEMBL"] = "ENSG00000274585"
        splicing_factors.loc[splicing_factors["GENE"]=="EIF3A","ENSEMBL"] = "ENSG00000107581"
        splicing_factors.loc[splicing_factors["GENE"]=="EIF3A","ENTREZ"] = 8661.0
        
        # add info
        splicing_factors["in_rogalska"] = splicing_factors["GENE"].isin(rogalska["GENE"])
        splicing_factors["in_hegele"] = splicing_factors["GENE"].isin(hegele["GENE"])
        splicing_factors["in_seiler"] = splicing_factors["GENE"].isin(seiler["GENE"])
        splicing_factors["in_papasaikas"] = splicing_factors["GENE"].isin(papasaikas["GENE"])
        splicing_factors["in_head"] = splicing_factors["GENE"].isin(head["GENE"])
        splicing_factors["in_spliceosome_db"] = splicing_factors["GENE"].isin(spliceosome_db["GENE"])
        splicing_factors["in_go_sf"] = splicing_factors["ENSEMBL"].isin(go_rna_splicing)
        splicing_factors["in_go_rbp"] = splicing_factors["ENSEMBL"].isin(go_rna_binding)
        
        # add spliceosome DB complex info
        splicing_factors = pd.merge(
            splicing_factors, 
            spliceosome_db.rename(columns={
                "Complex":"spliceosome_db_complex","Class / family":"spliceosome_db_class"
            })[["ENSEMBL","spliceosome_db_complex","spliceosome_db_class"]].drop_duplicates().dropna(), 
            on="ENSEMBL",
            how="left"
        )
        
        # save
        os.makedirs(output[0], exist_ok=True)
        splicing_factors.to_csv(os.path.join(output[0],"splicing_factors.tsv"), index=False, sep="\t")
        splicing_factors[["ENSEMBL"]].to_csv(os.path.join(output[0],"splicing_factors-ensembl.txt"), index=False, sep="\t", header=None)
        splicing_factors[["GENE"]].to_csv(os.path.join(output[0],"splicing_factors-symbol.txt"), index=False, sep="\t", header=None)
        
        print("Done!")
    
    
rule preprocess_ccle:
    input:
        sample_info = os.path.join(RAW_DIR,"DepMap","achilles_ccle","sample_info.csv"),
        ccle_cancer_types = os.path.join(RAW_DIR,"articles","Yu2019","ccle_metadata.xls"),
        sample_annotation = os.path.join(RAW_DIR,"CCLE","ENA_filereport-PRJNA523380-CCLE.tsv"),
        psi = os.path.join(RAW_DIR,'CCLE','vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,'CCLE','vast_out','TPM-hg38-1019.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","CCLE.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','CCLE-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','CCLE-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','CCLE-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','CCLE-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','CCLE.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        sample_info = pd.read_csv(input.sample_info)
        cancer_types = pd.read_excel(input.ccle_cancer_types)
        sample_annot = pd.read_table(input.sample_annotation)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        ## preprocess
        sample_annot = sample_annot.rename(
            columns={"sample_alias": "CCLE_Name"}
        )
        sample_annot = sample_annot.loc[sample_annot["library_strategy"] == "RNA-Seq"]
        cancer_types = cancer_types.rename(
            columns={"CCLE_name": "CCLE_Name", "disease": "cancer_type"}
        )

        ## combine
        metadata = pd.merge(sample_info, cancer_types, on="CCLE_Name", how="left")
        metadata = pd.merge(
            metadata,
            sample_annot[["run_accession", "CCLE_Name"]],
            on="CCLE_Name",
            how="left",
        )   
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        ## rename columns
        psis = {
            e: psis[e].rename(
                columns=metadata.set_index("run_accession")["DepMap_ID"].to_dict()
                ).copy() 
            for e in event_types
        }
        genexpr = genexpr.rename(
            columns=metadata.set_index("run_accession")["DepMap_ID"].to_dict()
        ).copy()
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")        

        
rule preprocess_CCLE_mutations:
    input:
        mutations = os.path.join(RAW_DIR,"DepMap","achilles_ccle","CCLE_mutations.csv"),
        annot = os.path.join(RAW_DIR,"HGNC","gene_annotations.tsv.gz")
    output:
        mutations = os.path.join(PREP_DIR,"mutations","CCLE.tsv.gz")
    run:
        import pandas as pd
        
        mutations = pd.read_csv(input.mutations, low_memory=False)
        annot = pd.read_table(input.annot)
        
        annot = annot.rename(columns={"Approved symbol":"GENE","Ensembl gene ID":"ENSEMBL"})[["GENE","ENSEMBL"]]
        mutations = pd.merge(mutations, annot, left_on="Hugo_Symbol", right_on="GENE", how="left")
        
        mutations["sampleID"] = mutations["DepMap_ID"]
        mutations["mutation_effect"] = mutations["Variant_Classification"]
        
        # indicate aberrant mutations
        mutations_oi = [
            # CCLE
            "Frame_Shift_Del","Frame_Shift_Ins",
            "De_novo_Start_OutOfFrame", "Nonsense_Mutation"
        ]
        mutations["is_aberrant"] = mutations["mutation_effect"].isin(mutations_oi)
        
        mutations.to_csv(output.mutations, **SAVE_PARAMS)
        
        print("Done!")
        

rule preprocess_TCGA:
    input:
        metadata = os.path.join(RAW_DIR,"TCGA","metadata","{cancer}.tsv.gz"),
        psi = os.path.join(RAW_DIR,"TCGA","{cancer}","vast_out",'PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = lambda wildcards: os.path.join(RAW_DIR,"TCGA","{cancer}","vast_out",'TPM-hg38-{n_samples}.tab.gz').format(cancer='{cancer}', n_samples=TCGA_N_SAMPLES[wildcards.cancer]),
        counts = lambda wildcards: os.path.join(RAW_DIR,"TCGA","{cancer}","vast_out",'COUNTS-hg38-{n_samples}.tab.gz').format(cancer='{cancer}', n_samples=TCGA_N_SAMPLES[wildcards.cancer])
    output:
        metadata = os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,"event_psi","{cancer}-EX.tsv.gz"),
        psi_ALTA = os.path.join(PREP_DIR,"event_psi","{cancer}-ALTA.tsv.gz"),
        psi_ALTD = os.path.join(PREP_DIR,"event_psi","{cancer}-ALTD.tsv.gz"),
        psi_INT = os.path.join(PREP_DIR,"event_psi","{cancer}-INT.tsv.gz"),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','{cancer}.tsv.gz'),
        counts = os.path.join(PREP_DIR,'genexpr_counts','{cancer}.tsv.gz')
    run:
        import gc
        import pandas as pd
        import numpy as np

        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        counts = pd.read_table(input.counts, index_col=[0,1])

        gc.collect()

        # preprocess metadata
        ## drop FFPE samples
        metadata = metadata.loc[~metadata["is_ffpe"],:].copy()

        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]

        ## remove vast-tools' suffix
        psi.columns = [c.replace("_1","") for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}        
        
        ## keep only the replicate with less missing values for each patient
        duplicated_sampleIDs = metadata.loc[metadata.duplicated('sampleID'),'sampleID'].unique()
        if len(duplicated_sampleIDs)>0:
            duplicated_file_ids = metadata.set_index('sampleID').loc[duplicated_sampleIDs,'file_id']

            # decide which sample to keep based on their missing values
            nan_count = psi[duplicated_file_ids].isnull().sum()
            file_ids_todrop = {sampleID: list(nan_count[duplicated_file_ids[sampleID]].sort_values().index[1:])
                               for sampleID in duplicated_file_ids.index.unique()}
            # drop samples
            file_ids_todrop = sum(list(file_ids_todrop.values()),[]) # unlist
            metadata = metadata.loc[~metadata['file_id'].isin(file_ids_todrop)]
            
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        counts.columns = [c.replace('_1','') for c in counts.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
            
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["file_id"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            ).intersection(
                counts.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        counts = counts[common_samples].copy()
        metadata = metadata.loc[metadata["file_id"].isin(common_samples)]
        
        ## rename columns
        psis = {
            e: psis[e].rename(
                columns=metadata.set_index("file_id")["sampleID"].to_dict()
                ).copy() 
            for e in event_types
        }
        genexpr = genexpr.rename(
            columns=metadata.set_index("file_id")["sampleID"].to_dict()
        ).copy()        
        counts = counts.rename(
            columns=metadata.set_index("file_id")["sampleID"].to_dict()
        ).copy()        
            
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)
        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        ## counts
        counts.reset_index().drop(columns='NAME').to_csv(output.counts, **SAVE_PARAMS)
        
        print("Done!")

        
rule preprocess_TCGA_mutations:
    input:
        mutations = os.path.join(RAW_DIR,"UCSCXena","GDC","snv","GDC-PANCAN.mutect2_snv.tsv.gz"),
        metadata = os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz"),
        annot = os.path.join(RAW_DIR,"HGNC","gene_annotations.tsv.gz")
    output:
        mutations = os.path.join(PREP_DIR,"mutations","{cancer}.tsv.gz")
    run:
        import pandas as pd
        
        mutations = pd.read_table(input.mutations, low_memory=False)
        metadata = pd.read_table(input.metadata)
        annot = pd.read_table(input.annot)
        
        # prep
        mutations["sampleID"] = mutations["Sample_ID"].str[:15]
        
        # subset
        mutations = mutations.loc[mutations["sampleID"].isin(metadata["sampleID"])].copy()
        
        # add ENSEMBL
        annot = annot.rename(columns={"Approved symbol":"GENE","Ensembl gene ID":"ENSEMBL"})[["GENE","ENSEMBL"]]
        mutations = pd.merge(mutations, annot, left_on="gene", right_on="GENE", how="left")
        
        mutations["mutation_effect"] = mutations["effect"]
        
        # indicate aberrant mutations
        mutations["is_aberrant"] = (
            mutations["mutation_effect"].str.contains("frameshift") |
            mutations["mutation_effect"].str.contains("stop_gained") |
            mutations["mutation_effect"].str.contains("start_lost")
        )        
        
        # save
        mutations.to_csv(output.mutations, **SAVE_PARAMS)
        
        print("Done!")


rule merge_tcga_metadata:
    input:
        metadatas = [os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz").format(cancer=c) for c in CANCER_TYPES]
    output:
        metadata = os.path.join(PREP_DIR,"metadata","PANCAN.tsv.gz")
    run:
        import pandas as pd
        
        metadata = pd.concat([pd.read_table(f) for f in input.metadatas])
        
        metadata.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_encorekd:
    input:
        metadata = os.path.join(RAW_DIR,'ENCODE','ENCORE','metadata','ENCORE.tsv'),
        psi = os.path.join(RAW_DIR,'ENCODE','ENCORE','vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,'ENCODE','ENCORE','vast_out','TPM-hg38-1097.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,'metadata','ENCOREKD.tsv.gz'),
        psi_EX = os.path.join(PREP_DIR,'event_psi','ENCOREKD-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','ENCOREKD-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','ENCOREKD-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','ENCOREKD-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','ENCOREKD.tsv.gz')
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        print("Processing metadata...")
        ## sample names
        metadata["sampleID"] = metadata["dbxrefs"].str.replace("SRA:","")
        ## cell lines info
        metadata["cell_line"] = metadata["Biosample term name"]
        depmapids = {"K562":"ACH-000551", "HepG2":"ACH-000739"}
        metadata["DepMap_ID"] = [depmapids[c] for c in metadata["cell_line"]]
        ## perturbation info
        metadata["PERT_GENE"] = metadata["Experiment target"].str.replace("-human","")
        gene_annot = genexpr.index.to_frame().rename(columns={"ID":"PERT_ENSEMBL", "NAME":"PERT_GENE"})
        metadata = pd.merge(metadata, gene_annot, how="left", on="PERT_GENE")
        metadata["PERT_TYPE"] = "KNOCKDOWN"
        ## experiment
        metadata["experiment"] = metadata["Experiment accession"]
        ## replicate
        metadata["replicate"] = metadata["Biological replicate(s)"]
        
        ## controls
        ctls_exps = []
        ctls_samps = []
        for idx, row in metadata.iterrows():
            if isinstance(row["Controlled by"], str):
                # get file accession controls
                accs = row["Controlled by"]\
                        .replace("files","")\
                        .replace("/","")\
                        .replace(" ","")\
                        .split(",")
                idx = metadata["File accession"].isin(accs)

                # get experiment accession
                exps = metadata.loc[idx, "experiment"].unique()
                
                # get sample accession
                samps = metadata.loc[idx, "sampleID"].unique()
                
                # save
                exps = ','.join(np.sort(exps))
                samps = ','.join(np.sort(samps))
                ctls_exps.append(exps)
                ctls_samps.append(samps)
            else:
                ctls_exps.append(np.nan)
                ctls_samps.append(np.nan)
        metadata["control_experiment"] = ctls_exps
        metadata["control_samples"] = ctls_samps
        
        cols_oi = ['sampleID','cell_line', 'DepMap_ID', 'PERT_GENE', 'PERT_ENSEMBL', 'PERT_TYPE', 'experiment', 
                   'control_experiment', 'control_samples','replicate']
        metadata = metadata[cols_oi].drop_duplicates()
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)
        
        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_encoreko:
    input:
        metadata = os.path.join(RAW_DIR,'ENCODE','ENCORE','CRISPRKO','metadata','CRISPRKO.tsv'),
        psi = os.path.join(RAW_DIR,'ENCODE','ENCORE','CRISPRKO','vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,'ENCODE','ENCORE','CRISPRKO','vast_out','TPM-hg38-924.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,'metadata','ENCOREKO.tsv.gz'),
        psi_EX = os.path.join(PREP_DIR,'event_psi','ENCOREKO-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','ENCOREKO-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','ENCOREKO-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','ENCOREKO-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','ENCOREKO.tsv.gz')
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        print("Processing metadata...")
        ## sample names
        metadata["sampleID"] = metadata["dbxrefs"].str.replace("SRA:","")
        ## cell lines info
        metadata["cell_line"] = metadata["Biosample term name"]
        depmapids = {"K562":"ACH-000551", "HepG2":"ACH-000739"}
        metadata["DepMap_ID"] = [depmapids[c] for c in metadata["cell_line"]]
        ## perturbation info
        metadata["PERT_GENE"] = metadata["Experiment target"].str.replace("-human","")
        gene_annot = genexpr.index.to_frame().rename(columns={"ID":"PERT_ENSEMBL", "NAME":"PERT_GENE"})
        metadata = pd.merge(metadata, gene_annot, how="left", on="PERT_GENE")
        metadata["PERT_TYPE"] = "KNOCKOUT"
        ## experiment
        metadata["experiment"] = metadata["Experiment accession"]
        ## replicate
        metadata["replicate"] = metadata["Biological replicate(s)"]
        
        ## controls
        ctls_exps = []
        ctls_samps = []
        for idx, row in metadata.iterrows():
            if isinstance(row["Controlled by"], str):
                # get file accession controls
                accs = row["Controlled by"]\
                        .replace("files","")\
                        .replace("/","")\
                        .replace(" ","")\
                        .split(",")
                idx = metadata["File accession"].isin(accs)

                # get experiment accession
                exps = metadata.loc[idx, "experiment"].unique()
                
                # get sample accession
                samps = metadata.loc[idx, "sampleID"].unique()
                
                # save
                exps = ','.join(np.sort(exps))
                samps = ','.join(np.sort(samps))
                ctls_exps.append(exps)
                ctls_samps.append(samps)
            else:
                ctls_exps.append(np.nan)
                ctls_samps.append(np.nan)
        metadata["control_experiment"] = ctls_exps
        metadata["control_samples"] = ctls_samps
        
        cols_oi = ['sampleID','cell_line', 'DepMap_ID', 'PERT_GENE', 'PERT_ENSEMBL', 'PERT_TYPE', 'experiment', 
                   'control_experiment', 'control_samples','replicate']
        metadata = metadata[cols_oi].drop_duplicates()
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)
        
        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
rule diff_tpm_encore:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','{dataset}.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','{dataset}.tsv.gz')
    output:
        diff_tpm = os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}',"{cell_line}",'log2_fold_change_tpm.tsv.gz')
    params:
        cell_line = "{cell_line}"
    run:
        import pandas as pd
        import numpy as np
        
        metadata = pd.read_table(input.metadata)
        genexpr = pd.read_table(input.genexpr, index_col=0)
        cell_line = params.cell_line
        
        # subset by cell line
        metadata = metadata.loc[metadata["cell_line"]==cell_line].copy()
        
        # log transform (already done)
        #genexpr = np.log2(genexpr + 1)
        #genexpr.columns = [c.replace("_1","") for c in genexpr.columns]
        
        # as the difference between conditions and the mean of the conditions
        diff_tpm = {}
        for sample_oi in metadata["sampleID"]:
            # get the controls of the sample
            ctls = metadata.loc[metadata["sampleID"]==sample_oi, "control_samples"].values[0]
            
            # controls will be np.nan
            if isinstance(ctls,str):
                ctls = ctls.split(",")
                tpm_ctls = genexpr[ctls].mean(axis=1)
                
                # compute log2 fold-change
                dtpm = genexpr[sample_oi] - tpm_ctls
                
                diff_tpm[sample_oi] = dtpm
                
                del dtpm, tpm_ctls, ctls
                
        diff_tpm = pd.DataFrame(diff_tpm)
        
        # save
        diff_tpm.reset_index().to_csv(output.diff_tpm, sep="\t", index=False, compression="gzip")
        
        print("Done!")
        
        
rule prepare_ground_truth_pert_logFCtpm:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','{dataset}.tsv.gz'),
        diff_tpm = os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}',"{cell_line}",'log2_fold_change_tpm.tsv.gz')
    params:
        cell_line = "{cell_line}",
        dataset = "{dataset}"
    output:
        diff_tpm = os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'log2_fold_change_tpm.tsv.gz')
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        diff_tpm = pd.read_table(input.diff_tpm, index_col=0)
        cell_line = params.cell_line
        dataset = params.dataset
        
        # drop control samples
        metadata = metadata.loc[~metadata["PERT_ENSEMBL"].isnull()].copy()
        
        # subset by cell line
        metadata = metadata.loc[metadata["cell_line"]==cell_line].copy()
        # make perturbation id
        metadata["study_accession"] = dataset
        metadata["PERT_ID"] = metadata[
            ["study_accession","cell_line","PERT_ENSEMBL","PERT_TYPE"]
        ].apply(lambda row: '___'.join(row.values.astype(str)), axis=1)  
        
        diff_tpms = []
        for pert_id in metadata["PERT_ID"].unique():
            samples_oi = metadata.loc[metadata["PERT_ID"]==pert_id, "sampleID"]
            
            diff_tpm_avg = diff_tpm[samples_oi].mean(axis=1)
            diff_tpm_avg.name = pert_id
            diff_tpms.append(diff_tpm_avg)
            
        diff_tpms = pd.concat(diff_tpms, axis=1)
        
        diff_tpms.reset_index().to_csv(output.diff_tpm, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule delta_psi_encore:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','{dataset}.tsv.gz'),
        psi = os.path.join(PREP_DIR,'event_psi','{dataset}-{event_type}.tsv.gz')
    output:
        delta_psi = os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}','{cell_line}','delta_psi-{event_type}.tsv.gz'),
    params:
        cell_line = "{cell_line}"
    run:
        import pandas as pd
        import numpy as np
        
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        cell_line = params.cell_line
        
        # subset by cell line
        metadata = metadata.loc[metadata["cell_line"]==cell_line].copy()
        
        # delta PSI as the difference between conditions and the mean of the conditions
        delta_psi = {}
        for sample_oi in metadata["sampleID"]:
            # get the controls of the sample
            ctls = metadata.loc[metadata["sampleID"]==sample_oi, "control_samples"].values[0]
            
            # controls will be np.nan
            if isinstance(ctls,str):
                ctls = ctls.split(",")
                psi_ctls = psi[ctls].mean(axis=1)
                
                # compute delta PSI
                dpsi = psi[sample_oi] - psi_ctls
                
                delta_psi[sample_oi] = dpsi
                
                del dpsi, psi_ctls, ctls
                
        
        delta_psi = pd.DataFrame(delta_psi)
        
        # save
        delta_psi.reset_index().to_csv(output.delta_psi, sep="\t", index=False, compression="gzip")
        
        print("Done!")
        
        
rule prepare_ground_truth_pert_dpsi:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','{dataset}.tsv.gz'),
        dpsi = os.path.join(PREP_DIR,'pert_transcriptomes','{dataset}',"{cell_line}",'{dpsi_type}-{event_type}.tsv.gz'),
    params:
        cell_line = "{cell_line}",
        dataset = "{dataset}",
    output:
        dpsi = os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'{dpsi_type}-{event_type}.tsv.gz'),
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        dpsi = pd.read_table(input.dpsi, index_col=0)
        cell_line = params.cell_line
        dataset = params.dataset
        
        # drop control samples
        metadata = metadata.loc[~metadata["PERT_ENSEMBL"].isnull()].copy()
        
        # subset by cell line
        metadata = metadata.loc[metadata["cell_line"]==cell_line].copy()
        
        # make perturbation id
        metadata["study_accession"] = dataset
        metadata["PERT_ID"] = metadata[
            ["study_accession","cell_line","PERT_ENSEMBL","PERT_TYPE"]
        ].apply(lambda row: '___'.join(row.values.astype(str)), axis=1)        
        dpsis = []
        
        for pert_id in metadata["PERT_ID"].unique():
            samples_oi = metadata.loc[metadata["PERT_ID"]==pert_id, "sampleID"]
            
            dpsi_avg = dpsi[samples_oi].mean(axis=1)
            dpsi_avg.name = pert_id
            dpsis.append(dpsi_avg)
            
        dpsis = pd.concat(dpsis, axis=1)

        dpsis.reset_index().to_csv(output.dpsi, **SAVE_PARAMS)
        
        print("Done!")

        
rule impute_psi:
    input:
        os.path.join(PREP_DIR,'event_psi','{dataset}-{event_type}.tsv.gz')
    output:
        os.path.join(PREP_DIR,'event_psi_imputed','{dataset}-{event_type}.tsv.gz')
    params:
        method = 'knn',
        method_kws = '\'{"n_neighbors":5}\'',
        features_as_rows = True
    threads: 1
    resources:
        # runtime = 3600*6, # h 
        runtime = 60*24, # h 
        memory = 300, # GB
    shell:
        """
        python scripts/impute_nan.py \
                    --input_file={input} \
                    --output_file={output} \
                    --method={params.method} \
                    --method_kws={params.method_kws} \
                    --features_as_rows={params.features_as_rows}
        """
        
rule inject_missing_values:
    input:
        psi = os.path.join(PREP_DIR,'event_psi','{dataset}-{event_type}.tsv.gz')
    output:
        psi = os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}.tsv.gz')
    run:
        import pandas as pd
        import numpy as np
        
        # load
        psi = pd.read_table(input.psi, index_col=0)
        
        # inject
        def inject_missing_values(df, missing_percentage=0.2):
            df_copy = df.copy()
            for col in df_copy.columns:
                # Get indices where values are not already missing
                non_missing_indices = df_copy.loc[df_copy[col].notnull()].index
                # Calculate the number of missing values to inject
                num_missing = int(len(non_missing_indices) * missing_percentage)
                # Randomly select indices to set as missing
                missing_indices = np.random.choice(non_missing_indices, num_missing, replace=False)
                # Inject missing values
                df_copy.loc[missing_indices, col] = np.nan
            return df_copy
        
        psi = inject_missing_values(psi)
        
        # save
        psi.reset_index().to_csv(output.psi, **SAVE_PARAMS)
        
        print("Done!")
        
    
rule psi_imputation_benchmark:
    input:
        os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}.tsv.gz')
    output:
        os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}-k{k}.tsv.gz')
    params:
        method = 'knn',
        method_kws = '\'{"n_neighbors":{k}}\'',
        features_as_rows = True
    threads: 1
    resources:
        # runtime = 3600*6, # h 
        runtime = 60*24, # h 
        memory = 300, # GB
    shell:
        """
        python scripts/impute_nan.py \
                    --input_file={input} \
                    --output_file={output} \
                    --method={params.method} \
                    --method_kws={params.method_kws} \
                    --features_as_rows={params.features_as_rows}
        """
        
        
rule make_summary_stats_splicing:
    input:
        omic = os.path.join(PREP_DIR,"event_psi_imputed","{dataset}-EX.tsv.gz")
    output:
        os.path.join(PREP_DIR,"summary_stats","event_psi_imputed","{dataset}-EX.tsv.gz")
    run:
        import pandas as pd
        from scipy import stats
        
        def get_summary_stats(df, label):
            summary_stats = {
                label + "_mean": df.mean(axis=1),
                label + "_median": df.median(axis=1),
                label + "_std": df.std(axis=1),
                label + "_std_ddof0": df.std(axis=1, ddof=0),
                label + "_mad": df.apply(stats.median_abs_deviation, axis=1),
                label + "_q25": df.quantile(0.25, axis=1),
                label + "_q75": df.quantile(0.75, axis=1),
            }
            return summary_stats
        
        # load
        omic = pd.read_table(input.omic, index_col=0)
        
        # get summary stats
        summary_stats = pd.DataFrame(get_summary_stats(omic, "EVENT"))
        
        # save
        summary_stats.reset_index().to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        

rule make_summary_stats_genexpr:
    input:
        omic = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz")
    output:
        os.path.join(PREP_DIR,"summary_stats","genexpr_tpm","{dataset}.tsv.gz")
    run:
        import pandas as pd
        from scipy import stats
        
        def get_summary_stats(df, label):
            summary_stats = {
                label + "_mean": df.mean(axis=1),
                label + "_median": df.median(axis=1),
                label + "_std": df.std(axis=1),
                label + "_std_ddof0": df.std(axis=1, ddof=0),
                label + "_mad": df.apply(stats.median_abs_deviation, axis=1),
                label + "_q25": df.quantile(0.25, axis=1),
                label + "_q75": df.quantile(0.75, axis=1),
            }
            return summary_stats
        
        # load
        omic = pd.read_table(input.omic, index_col=0)
        
        # get summary stats
        summary_stats = pd.DataFrame(get_summary_stats(omic, "ENSEMBL"))
        
        # save
        summary_stats.reset_index().to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_demeter2:
    input:
        demeter2 = os.path.join(RAW_DIR,"DepMap","demeter2","D2_combined_gene_dep_scores.csv"),
        metadata = os.path.join(RAW_DIR,'DepMap','achilles_ccle','sample_info.csv')
    output:
        os.path.join(PREP_DIR,"demeter2","CCLE.tsv.gz")
    run:
        import pandas as pd
        
        # load
        demeter2 = pd.read_csv(input.demeter2, index_col=0)
        metadata = pd.read_csv(input.metadata)
        
        # PREPROCESS
        ## strip gene names
        demeter2.index = [symbol for symbol, entrez in demeter2.index.str.split(" ")]

        ## rename samples
        demeter2 = demeter2.rename(
            columns=metadata.set_index("CCLE_Name")["DepMap_ID"].to_dict()
        )

        ## drop rows missing many values
        is_na = demeter2.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 50
        demeter2 = demeter2.loc[to_keep].copy()
        
        # save
        demeter2.reset_index().to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_Nijhuis2020:
    input:
        metadata = os.path.join(SUPPORT_DIR,"ENA_filereport-PRJNA673205-Nijhuis2020.tsv"),
        metadata_proteomics = os.path.join(SUPPORT_DIR,"Nijhuis2020-metadata-proteomics.tsv"),
        psi = os.path.join(RAW_DIR,"articles","Nijhuis2020",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"articles","Nijhuis2020",'vast_out','TPM-hg38-6.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","Nijhuis2020.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','Nijhuis2020-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','Nijhuis2020-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','Nijhuis2020-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','Nijhuis2020-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','Nijhuis2020.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        metadata_proteomics = pd.read_table(input.metadata_proteomics)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        metadata["sampleID"] = metadata["run_accession"]
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        ## append metadata proteomics
        metadata = pd.concat([metadata, metadata_proteomics])
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_Lu2021:
    input:
        metadata = os.path.join(SUPPORT_DIR,"ENA_filereport-PRJNA683080-Lu2021.tsv"),
        psi = os.path.join(RAW_DIR,"articles","Lu2021",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"articles","Lu2021",'vast_out','TPM-hg38-27.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","Lu2021.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','Lu2021-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','Lu2021-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','Lu2021-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','Lu2021-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','Lu2021.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        metadata["sampleID"] = metadata["run_accession"]
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_ena_datasets:
    input:
        metadata = os.path.join(SUPPORT_DIR,"ENA_filereport-{dataset}.tsv"),
        psi = os.path.join(RAW_DIR,"ENA","{dataset}",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = lambda wildcards: os.path.join(RAW_DIR,"ENA","{dataset}",'vast_out','TPM-hg38-{n_samples}.tab.gz').format(n_samples=N_SAMPLES_ENA[wildcards.dataset], dataset=wildcards.dataset)
    output:
        metadata = os.path.join(PREP_DIR,"metadata","{dataset}.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','{dataset}-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','{dataset}-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','{dataset}-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','{dataset}-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','{dataset}.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        metadata["sampleID"] = metadata["run_accession"]
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")


rule preprocess_ena_splicing_factors:
    input:
        metadata = os.path.join(SUPPORT_DIR,'ENA_filereport-selected_sf_experiments_handcurated_w_control_samples.tsv'),
        metadata_groups = os.path.join(SUPPORT_DIR,"ENA_filereport-selected_sf_experiments_handcurated-low_read_count_groups.tsv"),
        psi = os.path.join(RAW_DIR,"ENA","splicing_factors",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"ENA","splicing_factors",'vast_out','TPM-hg38-1597.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","ENASFS.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','ENASFS-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','ENASFS-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','ENASFS-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','ENASFS-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','ENASFS.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        metadata_groups = pd.read_table(input.metadata_groups)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        ## not grouped metadata
        metadata_not_merged = metadata.loc[~metadata["run_accession"].isin(metadata_groups["run_accession"])].copy()
        metadata_not_merged["sampleID"] = metadata_not_merged["run_accession"]
        metadata_not_merged["control_samples"] = metadata_not_merged["control_samples"].str.replace(",","||")
        
        ## grouped metadata
        ### subset
        metadata_merged = metadata.loc[metadata["run_accession"].isin(metadata_groups["run_accession"])].copy()
        metadata_merged = pd.merge(metadata_merged, metadata_groups[["group_label","run_accession"]], how="left", on="run_accession")
        ### control_samples at the group level
        control_samples_group = []
        for ctls in metadata_merged["control_samples"].dropna().unique():
            # get run accessions
            run_accessions = ctls.split(",")
            
            # get corresponding group labels
            group_labs = metadata_merged.loc[metadata_merged["run_accession"].isin(run_accessions),"group_label"].unique()
            
            # join group labels
            ctls_group = "||".join(group_labs)
            
            # save
            control_samples_group.append({
                "control_samples": ctls,
                "control_samples_group": ctls_group
            })
        control_samples_group = pd.DataFrame(control_samples_group)
        metadata_merged = pd.merge(metadata_merged, control_samples_group, on="control_samples", how="left")
        metadata_merged["control_samples"] = metadata_merged["control_samples_group"]
        
        ### columns to keep
        cols_oi = [
            "study_accession","cell_line_name","condition","pert_time",
            "pert_time_units","pert_concentration","pert_concentration_units",
            'found_sfs_in_run_alias', 'found_sfs_in_sample_alias', 'found_sfs_in_sample_title', 
            'found_sfs_in_experiment_title', 'found_sfs_in_study_title', 'is_match', 'group_label',
            "DepMap_ID","CCLE_Name","PERT_GENE","PERT_ENSEMBL","IS_USEFUL","comments","PERT_TYPE",
            "control_samples"
        ]
        metadata_merged = metadata_merged[cols_oi].drop_duplicates().copy()
        metadata_merged["sampleID"] = metadata_merged["group_label"]
        
        ## combine metadatas
        metadata = pd.concat([metadata_not_merged, metadata_merged])
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["sampleID"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["sampleID"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule diff_tpm_ena_splicing_factors:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','ENASFS.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','ENASFS.tsv.gz')
    output:
        diff_tpm = os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','log2_fold_change_tpm.tsv.gz')
    run:
        import pandas as pd
        import numpy as np
        
        metadata = pd.read_table(input.metadata)
        genexpr = pd.read_table(input.genexpr, index_col=0)
        
        # as the difference between conditions and the mean of the conditions
        diff_tpm = {}
        for sample_oi in metadata["sampleID"]:
            # get the controls of the sample
            ctls = metadata.loc[metadata["sampleID"]==sample_oi, "control_samples"].values[0]
            
            # controls will be np.nan
            if isinstance(ctls,str):
                ctls = ctls.split("||")
                tpm_ctls = genexpr[ctls].mean(axis=1)
                
                # compute log2 fold-change
                dtpm = genexpr[sample_oi] - tpm_ctls
                
                diff_tpm[sample_oi] = dtpm
                
                del dtpm, tpm_ctls, ctls
                
        diff_tpm = pd.DataFrame(diff_tpm)
        
        # save
        diff_tpm.reset_index().to_csv(output.diff_tpm, sep="\t", index=False, compression="gzip")
        
        print("Done!")
        
        
rule prepare_ground_truth_pert_logFCtpm_ena_splicing_factors:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','ENASFS.tsv.gz'),
        diff_tpm = os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','log2_fold_change_tpm.tsv.gz')
    output:
        diff_tpm = os.path.join(PREP_DIR,'ground_truth_pert','ENASFS','log2_fold_change_tpm.tsv.gz')
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        diff_tpm = pd.read_table(input.diff_tpm, index_col=0)

        # drop control samples
        metadata = metadata.loc[~metadata["PERT_ENSEMBL"].isnull()].copy()
        metadata["PERT_ID"] = metadata[
            ["study_accession","cell_line_name","PERT_ENSEMBL","PERT_TYPE"]
        ].apply(lambda row: '___'.join(row.values.astype(str)), axis=1)
        
        diff_tpms = []
        for pert_id in metadata["PERT_ID"].unique():
            samples_oi = metadata.loc[metadata["PERT_ID"]==pert_id, "sampleID"]
            
            diff_tpm_avg = diff_tpm[samples_oi].mean(axis=1)
            diff_tpm_avg.name = pert_id
            diff_tpms.append(diff_tpm_avg)

        diff_tpms = pd.concat(diff_tpms, axis=1)

        diff_tpms.reset_index().to_csv(output.diff_tpm, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule delta_psi_ena_splicing_factors:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','ENASFS.tsv.gz'),
        psi = os.path.join(PREP_DIR,'event_psi','ENASFS-{event_type}.tsv.gz')
    output:
        delta_psi = os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','delta_psi-{event_type}.tsv.gz')
    run:
        import pandas as pd
        
        # load
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        
        # delta PSI as the difference between conditions and the mean of the conditions
        delta_psi = {}
        for sample_oi in metadata["sampleID"]:
            # get the controls of the sample
            ctls = metadata.loc[metadata["sampleID"]==sample_oi, "control_samples"].values[0]
            
            # controls will be np.nan
            if isinstance(ctls,str):
                ctls = ctls.split("||")
                psi_ctls = psi[ctls].mean(axis=1)
                
                # compute delta PSI
                dpsi = psi[sample_oi] - psi_ctls
                
                delta_psi[sample_oi] = dpsi
                
                del dpsi, psi_ctls, ctls
                
        
        delta_psi = pd.DataFrame(delta_psi)
        
        # save
        delta_psi.reset_index().to_csv(output.delta_psi, **SAVE_PARAMS)
        
        print("Done!")

        
rule prepare_ground_truth_pert_dpsi_ena_splicing_factors:
    input:
        metadata = os.path.join(PREP_DIR,'metadata','ENASFS.tsv.gz'),
        dpsi = os.path.join(PREP_DIR,'pert_transcriptomes','ENASFS','delta_psi-{event_type}.tsv.gz')
    output:
        dpsi = os.path.join(PREP_DIR,'ground_truth_pert','ENASFS','delta_psi-{event_type}.tsv.gz')
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        dpsi = pd.read_table(input.dpsi, index_col=0)
        
        # drop control samples
        metadata = metadata.loc[~metadata["PERT_ENSEMBL"].isnull()].copy()
        metadata["PERT_ID"] = metadata[
            ["study_accession","cell_line_name","PERT_ENSEMBL","PERT_TYPE"]
        ].apply(lambda row: '___'.join(row.values.astype(str)), axis=1)
        
        dpsis = []
        for pert_id in metadata["PERT_ID"].unique():
            samples_oi = metadata.loc[metadata["PERT_ID"]==pert_id, "sampleID"]

            dpsi_avg = dpsi[samples_oi].mean(axis=1)
            dpsi_avg.name = pert_id
            dpsis.append(dpsi_avg)

        dpsis = pd.concat(dpsis, axis=1)

        dpsis.reset_index().to_csv(output.dpsi, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_CardosoMoreira2020:
    input:
        metadata = os.path.join(SUPPORT_DIR,"ENA_filereport-PRJEB26969-CardosoMoreira2020.tsv"),
        psi = os.path.join(RAW_DIR,"articles","CardosoMoreira2020",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"articles","CardosoMoreira2020",'vast_out','TPM-hg38-313.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","CardosoMoreira2020.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','CardosoMoreira2020-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','CardosoMoreira2020-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','CardosoMoreira2020-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','CardosoMoreira2020-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','CardosoMoreira2020.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        metadata["sampleID"] = metadata["run_accession"]
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_tumorigenesis:
    input:
        metadata = os.path.join(SUPPORT_DIR,'ENA_filereport-tumorigenesis.tsv'),
        metadata_groups = os.path.join(SUPPORT_DIR,"ENA_filereport-tumorigenesis-low_read_count_groups.tsv"),
        psi = os.path.join(RAW_DIR,"ENA","tumorigenesis",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"ENA","tumorigenesis",'vast_out','TPM-hg38-55.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","tumorigenesis.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','tumorigenesis-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','tumorigenesis-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','tumorigenesis-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','tumorigenesis-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','tumorigenesis.tsv.gz')
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        metadata_groups = pd.read_table(input.metadata_groups)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata
        ## not grouped metadata
        metadata_not_merged = metadata.loc[~metadata["run_accession"].isin(metadata_groups["run_accession"])].copy()
        metadata_not_merged["sampleID"] = metadata_not_merged["run_accession"]
        metadata_not_merged["control_samples"] = metadata_not_merged["control_samples"].str.replace(",","||")
        
        ## grouped metadata
        ### subset
        metadata_merged = metadata.loc[metadata["run_accession"].isin(metadata_groups["run_accession"])].copy()
        metadata_merged = pd.merge(metadata_merged, metadata_groups[["group_label","run_accession"]], how="left", on="run_accession")
        ### control_samples at the group level
        control_samples_group = []
        for ctls in metadata_merged["control_samples"].dropna().unique():
            # get run accessions
            run_accessions = ctls.split(",")
            
            # get corresponding group labels
            group_labs = metadata_merged.loc[metadata_merged["run_accession"].isin(run_accessions),"group_label"].unique()
            
            # join group labels
            ctls_group = "||".join(group_labs)
            
            # save
            control_samples_group.append({
                "control_samples": ctls,
                "control_samples_group": ctls_group
            })
        control_samples_group = pd.DataFrame(control_samples_group)
        metadata_merged = pd.merge(metadata_merged, control_samples_group, on="control_samples", how="left")
        metadata_merged["control_samples"] = metadata_merged["control_samples_group"]
        
        ### columns to keep
        cols_oi = [
            "study_accession","cell_line_name","condition","pert_time",
            "pert_time_units","pert_concentration","pert_concentration_units",
            'group_label',
            "DepMap_ID","CCLE_Name","PERT_GENE","PERT_ENSEMBL","PERT_TYPE",
            "control_samples"
        ]
        metadata_merged = metadata_merged[cols_oi].drop_duplicates().copy()
        metadata_merged["sampleID"] = metadata_merged["group_label"]
        
        ## combine metadatas
        metadata = pd.concat([metadata_not_merged, metadata_merged])
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["sampleID"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["sampleID"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_Riaz2017:
    input:
        metadata = os.path.join(SUPPORT_DIR,"ENA_filereport-PRJNA356761-Riaz2017.tsv"),
        survival = os.path.join(SUPPORT_DIR,"supplementary_tables_literature","Riaz2017-suptab2-mmc2.xlsx"),
        psi = os.path.join(RAW_DIR,"articles","Riaz2017",'vast_out','PSI-minN_1-minSD_0-noVLOW-min_ALT_use25-Tidy.tab.gz'),
        genexpr = os.path.join(RAW_DIR,"articles","Riaz2017",'vast_out','TPM-hg38-109.tab.gz')
    output:
        metadata = os.path.join(PREP_DIR,"metadata","Riaz2017.tsv.gz"),
        psi_EX = os.path.join(PREP_DIR,'event_psi','Riaz2017-EX.tsv.gz'),
        psi_ALTA = os.path.join(PREP_DIR,'event_psi','Riaz2017-ALTA.tsv.gz'),
        psi_ALTD = os.path.join(PREP_DIR,'event_psi','Riaz2017-ALTD.tsv.gz'),
        psi_INT = os.path.join(PREP_DIR,'event_psi','Riaz2017-INT.tsv.gz'),
        genexpr = os.path.join(PREP_DIR,'genexpr_tpm','Riaz2017.tsv.gz')      
    run:
        import gc
        import pandas as pd
        import numpy as np
        
        # load
        print("Loading data...")
        metadata = pd.read_table(input.metadata)
        survival = pd.read_excel(input.survival, skiprows=2)
        psi = pd.read_table(input.psi, index_col=0)
        genexpr = pd.read_table(input.genexpr, index_col=[0,1])
        
        gc.collect()
        
        # metadata       
        metadata["sampleID"] = metadata["run_accession"]
        
        # survival
        ## create common column names
        survival["patientID"] = survival["Patient"]
        survival["OS_event"] = survival["Dead/Alive\n(Dead = True)"]
        survival["OS_time"] = survival["Time to Death\n(weeks)"] * 7
        
        ## extras
        ### previous treatments
        survival["treatment_previous"] = survival["Cohort"]
        ### RECIST v1.1-defined complete response [CR] or partial response [PR]
        ### Neither PR nor PD criteria met [SD], 
        ### ≥ 25% increase, no CR, PR, or SD, new lesion (s), ≥ 25% increase in 1 lesion [PD]
        survival["treatment_response"] = survival["Response"]
        ### in the paper, they considered DEGs between responders (CR/PR) 
        ### compared with non-responders (PD) or patients with SD
        survival["is_responder"] = np.nan
        idx_responder = survival["treatment_response"].isin(["CR","PR"])
        idx_nonresponder = survival["treatment_response"].isin(["PD","SD"])
        survival.loc[idx_responder,"is_responder"] = True
        survival.loc[idx_nonresponder,"is_responder"] = False
        
        # merge with metadata
        metadata = pd.merge(metadata, survival, on="patientID", how="left")
        
        # correct condition
        metadata["treatment_status"] = metadata["condition"].copy()
        metadata["condition"] = np.nan
        
        idx = (metadata["treatment_status"]=="PRE") & (metadata["treatment_previous"]=="NIV3-NAIVE")
        metadata.loc[idx,"condition"] = "PRE_PD1"
        
        idx = (metadata["treatment_status"]=="PRE") & (metadata["treatment_previous"]=="NIV3-PROG")
        metadata.loc[idx,"condition"] = "PRE_CombPD1_CTLA4"
        
        idx = (metadata["treatment_status"]=="ON") & (metadata["treatment_previous"]=="NIV3-NAIVE")
        metadata.loc[idx,"condition"] = "ON_PD1"
        
        idx = (metadata["treatment_status"]=="ON") & (metadata["treatment_previous"]=="NIV3-PROG")
        metadata.loc[idx,"condition"] = "ON_CombPD1_CTLA4"        
        
        # PSI
        print("Processing PSI matrix...")
        ## drop empty rows
        is_na = psi.isnull()
        non_missing = is_na.shape[1] - is_na.sum(1)
        to_keep = non_missing >= 1
        psi = psi.loc[to_keep]
        
        ## remove vast-tools' suffix
        psi.columns = [c.replace('_1','') for c in psi.columns]
        
        ## split by event type
        event_types = ["EX","ALTA","ALTD","INT"]
        psis = {e: psi.loc[psi.index.str.contains(e)] for e in event_types}
        
        # TPM
        print("Processing TPM matrix...")
        ## remove vast-tools' suffix
        genexpr.columns = [c.replace('_1','') for c in genexpr.columns]
        
        ## log-transform
        genexpr = np.log2(genexpr + 1)
        
        # subset
        ## find common samples
        common_samples = list(
            set(metadata["run_accession"]).intersection(
                psis["EX"].columns
            ).intersection(
                genexpr.columns
            )
        )
        psis = {e: psis[e][common_samples].copy() for e in event_types}
        genexpr = genexpr[common_samples].copy()
        metadata = metadata.loc[metadata["run_accession"].isin(common_samples)]
        
        # save
        print("Saving...")
        ## metadata
        metadata.to_csv(output.metadata, **SAVE_PARAMS)

        ## PSIs
        psis["EX"].reset_index().to_csv(output.psi_EX, **SAVE_PARAMS)
        psis["ALTD"].reset_index().to_csv(output.psi_ALTD, **SAVE_PARAMS)
        psis["ALTA"].reset_index().to_csv(output.psi_ALTA, **SAVE_PARAMS)
        psis["INT"].reset_index().to_csv(output.psi_INT, **SAVE_PARAMS)
        
        ## TPMs
        genexpr.reset_index().drop(columns='NAME').to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule preprocess_stringdb:
    input:
        ppi = os.path.join(RAW_DIR,'STRINGDB','9606.protein.links.full.v11.5.txt.gz'),
        aliases = os.path.join(RAW_DIR,'STRINGDB','9606.protein.aliases.v11.5.txt.gz')
    output:
        os.path.join(PREP_DIR,'ppi','STRINGDB.tsv.gz')
    shell:
        """
        python scripts/preprocess_stringdb.py \
                    --raw_ppi_file={input.ppi} \
                    --raw_aliases_file={input.aliases} \
                    --prep_ppi_file={output}
        """

        
rule figures_eda:
    input:
        splicing_factors = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors.tsv"),
        metadata_encore_kd = os.path.join(PREP_DIR,"metadata","ENCOREKD.tsv.gz"),
        metadata_encore_ko = os.path.join(PREP_DIR,"metadata","ENCOREKO.tsv.gz"),
        kd_screen = os.path.join(SUPPORT_DIR,"kd_screen-symbol.txt"),
        ena_sfs = os.path.join(SUPPORT_DIR,"ENA_filereport-selected_sf_experiments_handcurated.tsv")
    output:
        directory(os.path.join(RESULTS_DIR,'figures','eda'))
    shell:
        """
        Rscript scripts/figures_eda.R \
                    --splicing_factors_file={input.splicing_factors} \
                    --metadata_encore_kd_file={input.metadata_encore_kd} \
                    --metadata_encore_ko_file={input.metadata_encore_ko} \
                    --kd_screen_file={input.kd_screen} \
                    --ena_sfs_file={input.ena_sfs} \
                    --figs_dir={output}
        """
        
        
rule figures_benchmark_imputation:
    input:
        original = os.path.join(PREP_DIR,'event_psi','CardosoMoreira2020-EX.tsv.gz'),
        synthetic = os.path.join(PREP_DIR,'event_psi_imputation_benchmark','CardosoMoreira2020-EX.tsv.gz'),
        imputed = [os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}-k{k}.tsv.gz').format(dataset="CardosoMoreira2020", event_type=e, k=k) 
                   for e in EVENT_TYPES for k in ["2","5","10","50","100"]]
    params:
        imputed_joined = ",".join(
            [os.path.join(PREP_DIR,'event_psi_imputation_benchmark','{dataset}-{event_type}-k{k}.tsv.gz').format(dataset="CardosoMoreira2020", event_type=e, k=k) 
             for e in EVENT_TYPES for k in ["2","5","10","50","100"]]
)
    output:
        directory(os.path.join(RESULTS_DIR,'figures','benchmark_imputation'))
    shell:
        """
        Rscript scripts/figures_benchmark_imputation.R \
                    --original_file={input.original} \
                    --synthetic_file={input.synthetic} \
                    --imputed_files={params.imputed_joined} \
                    --figs_dir={output}
        """        
        
        
rule split_event_psi_by_cancer_and_sample_type:
    input:
        metadata = os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz"),
        psi = os.path.join(PREP_DIR,"event_psi","{cancer}-EX.tsv.gz")
    output:
        psi = os.path.join(PREP_DIR,"event_psi","{cancer}-{sample}-EX.tsv.gz")
    params:
        cancer_type = "{cancer}",
        sample_type = "{sample}"
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        psi = pd.read_table(input.psi, index_col=0)
        cancer_type = params.cancer_type
        sample_type = params.sample_type
        
        idx = (metadata["cancer_type"]==cancer_type) & (metadata["sample_type_clean"]==sample_type)
        samples_oi = metadata.loc[idx,"sampleID"].values
        
        psi = psi[samples_oi]
        print(psi.shape)
        
        if psi.shape[1]>0:
            print("Saving...")
            psi.reset_index().to_csv(output.psi, **SAVE_PARAMS)
        
        print("Done!")


rule split_genexpr_tpm_by_cancer_and_sample_type:
    input:
        metadata = os.path.join(PREP_DIR,"metadata","{cancer}.tsv.gz"),
        genexpr = os.path.join(PREP_DIR,"genexpr_tpm","{cancer}.tsv.gz")
    output:
        genexpr = os.path.join(PREP_DIR,"genexpr_tpm","{cancer}-{sample}.tsv.gz")
    params:
        cancer_type = "{cancer}",
        sample_type = "{sample}"
    run:
        import pandas as pd
        
        metadata = pd.read_table(input.metadata)
        genexpr = pd.read_table(input.genexpr, index_col=0)
        cancer_type = params.cancer_type
        sample_type = params.sample_type
        
        idx = (metadata["cancer_type"]==cancer_type) & (metadata["sample_type_clean"]==sample_type)
        samples_oi = metadata.loc[idx,"sampleID"].values
        
        genexpr = genexpr[samples_oi]
        print(genexpr.shape)
        
        if genexpr.shape[1]>0:
            print("Saving...")
            genexpr.reset_index().to_csv(output.genexpr, **SAVE_PARAMS)
        
        print("Done!")
        

ruleorder: concatenate_tcga_splicing > split_event_psi_by_cancer_and_sample_type
rule concatenate_tcga_splicing:
    input:
        tables = lambda wildcards: [
            os.path.join(PREP_DIR,"event_psi","{cancer}-{sample}-EX.tsv.gz").format(cancer=c, sample=wildcards.sample) 
            for c in CANCER_TYPES_CONCAT[wildcards.sample]
        ]
    output:
        os.path.join(PREP_DIR,"event_psi","PANCAN-{sample}-EX.tsv.gz")
    run:
        import pandas as pd
        
        table = pd.concat([pd.read_table(f, index_col=0) for f in input.tables], axis=1)
        
        print(table.shape)
        
        table.reset_index().to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule concatenate_tcga_genexpr:
    input:
        tables = lambda wildcards: [
            os.path.join(PREP_DIR,"genexpr_tpm","{cancer}-{sample}.tsv.gz").format(cancer=c, sample=wildcards.sample) 
            for c in CANCER_TYPES_CONCAT[wildcards.sample]
        ]
    output:
        os.path.join(PREP_DIR,"genexpr_tpm","PANCAN-{sample}.tsv.gz")
    run:
        import pandas as pd
        
        table = []
        for f in input.tables:
            print(f)
            table.append(pd.read_table(f, index_col=0))
        table = pd.concat(table, axis=1)
        
        print(table.shape)
        
        table.reset_index().to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
    
    
rule map_clip_peaks_to_exons:
    input:
        peaks = os.path.join(RAW_DIR,'POSTAR3','human.txt.gz'),
        event_info = os.path.join(RAW_DIR,"VastDB","EVENT_INFO-hg38_noseqs.tsv")
    output:
        os.path.join(PREP_DIR,"clip_peaks_mapped","POSTAR3.tsv.gz")
    params:
        margin = 500
    run:
        import pandas as pd
        import pyranges as pr
        
        # load
        ## read peaks file
        col_names = [
            "Chromosome",
            "Start",
            "End",
            "peak_id",
            "Strand",
            "RBP",
            "experiment_method",
            "experiment_model",
            "experiment_id"
        ]
        peaks = pd.read_table(
            input.peaks,
            header=None,
            names=col_names
        )
        ## read vastdb events
        events = pd.read_table(input.event_info)
        ## params
        margin = params.margin
        
        # assign exons to peaks if a peak falls on the exon or one of its neighboring introns
        ## only event type of interest
        events = events.loc[events["EVENT"].str.contains("EX")].copy()
        ## process event coordinates
        events["Chromosome"] = (
            events["COORD_o"].str.split(":").str[0]
        )
        events["EVENT_start"] = (
            events["COORD_o"].str.split(":").str[1].str.split("-").str[0]
        ).astype("int")
        events["EVENT_end"] = (
            events["COORD_o"].str.split(":").str[1].str.split("-").str[1].astype("int")
        )
        events["Strand"] = (
            events["REF_CO"].str.split(":").str[2]
        )
        events["Start"] = events["EVENT_start"]
        events["End"] = events["EVENT_end"]
        
        # intersect
        ## prepare
        X = pr.PyRanges(
            events[["EVENT", "Chromosome", "Start", "End", "Strand"]],
            int64=True,
        )
        Y = pr.PyRanges(peaks, int64=True)
        ## join
        mapping = X.join(Y, slack=margin, how="left", report_overlap=True)
               
        # prepare outputs
        ## drop unmapped
        mapping = mapping.df.loc[~mapping.df["peak_id"].isin(["-1"])]
        ## rename
        mapping = mapping.rename(columns={
            "Start_b":"peak_start",
            "End_b":"peak_end"
        })
        
        # save
        mapping.to_csv(output[0], sep="\t", compression="gzip", index=False)
        
        print("Done!")
        
        
rule preprocess_splicinglore:
    input:
        folder = os.path.join(RAW_DIR,"SplicingLore"),
        event_info = os.path.join(RAW_DIR,"VastDB","EVENT_INFO-hg19_noseqs.tsv"),
        urls = os.path.join(SUPPORT_DIR, "SplicingLore-urls.txt")
    output:
        metadata = os.path.join(PREP_DIR,"metadata","SplicingLore.tsv.gz"),
        perts = os.path.join(PREP_DIR,"ground_truth_pert","SplicingLore","delta_psi-EX.tsv.gz")
    params:
        margin = 0
    run:
        import os
        import pandas as pd
        from urllib.parse import urlparse
        import pyranges as pr
        
        # load
        sl_dir = input.folder
        pert_files = [f for f in os.listdir(sl_dir, ) if f.endswith(".csv")]
        events = pd.read_table(input.event_info)
        urls = pd.read_table(input.urls, names=["url"])
        margin = params.margin
        
        # prep URLs
        urls["file_name"] = urls["url"].apply(lambda x: os.path.basename(urlparse(x).path))
        urls["experiment_id"] = urls["file_name"].str.replace(".csv","", regex=False)
        splits = urls["experiment_id"].str.split("_", expand=True)
        splits["PERT_GENE"] = splits[0]
        splits["study_accession"] = splits[1]
        splits["cell_line"] = splits[2]
        splits["experiment_id"] = splits[3]
        ## fix splits (for the 7 experiments perturbing multiple splicing factors)
        idx = ~splits[4].isnull()
        splits.loc[idx, "PERT_GENE"] = splits.loc[idx, [0,1]].apply(lambda x: ",".join(x[[0,1]]), axis=1)
        splits.loc[idx, "study_accession"] = splits.loc[idx, 2]
        splits.loc[idx, "cell_line"] = splits.loc[idx, 3]
        splits.loc[idx, "experiment_id"] = splits.loc[idx, 4]
        ## update urls
        cols_oi = ["PERT_GENE","study_accession","cell_line","experiment_id"] 
        urls[cols_oi] = splits[cols_oi]
        ## rename to metadata
        metadata = urls.copy()
        ## make PERT_ID
        metadata["PERT_ID"] = metadata["file_name"]
        
        # prep mapping to VASTDB
        ## only event type of interest
        events = events.loc[events["EVENT"].str.contains("EX")].copy()
        ## process event coordinates
        events["Chromosome"] = (
            events["COORD_o"].str.split(":").str[0]
        )
        events["EVENT_start"] = (
            events["COORD_o"].str.split(":").str[1].str.split("-").str[0]
        ).astype("int")
        events["EVENT_end"] = (
            events["COORD_o"].str.split(":").str[1].str.split("-").str[1].astype("int")
        )
        events["Strand"] = (
            events["REF_CO"].str.split(":").str[2]
        )
        events["Start"] = events["EVENT_start"]
        events["End"] = events["EVENT_end"]
        
        # get genes to add strand to perts
        gene_strand = events[["GENE","Strand"]].drop_duplicates()
        
        # load each perturbation, map and concatenate
        perts = []
        for f in pert_files:
            print(f"Processing {f} ...")
            # load
            pert = pd.read_csv(os.path.join(sl_dir,f), sep=";")
            
            # add gene
            pert["GENE"] = pert["Gene_exon"].str.split("_").str[0]
            
            # add experiment metadata
            pert["file_name"] = f
            pert = pd.merge(pert, metadata, on="file_name", how="left")
            
            # prep intersection
            pert["Chromosome"] = "chr"+pert["chr"].astype("str")
            pert["Start"] = pert["start"]
            pert["End"] = pert["end"]
            
            # intersect by exact exon match
            mapping = pd.merge(
                events[["EVENT","GENE", "Chromosome", "Start", "End", "Strand"]],
                pert, 
                on=["GENE","Chromosome","Start","End"],
                how="left"
            )
            
            # filter out unmapped exons
            mapping = mapping.loc[~mapping["Gene_exon"].isnull()]
            
            print("Mapped %s interactions out of %s." % (mapping.shape[0], pert.shape[0]))
            
            # store
            perts.append(mapping)
            
            del pert

        perts = pd.concat(perts)
        
        # save
        metadata.to_csv(output.metadata, **SAVE_PARAMS)
        perts.to_csv(output.perts, **SAVE_PARAMS)
        
        print("Done!")
