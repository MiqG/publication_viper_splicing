"""
Author: Miquel Anglada Girotto
Contact: miquelangladagirotto [at] gmail [dot] com

Outline
-------
1. Associations
    - Spearman correlation
    - Mutual information (ARACNe-AP)
    - Linear models

2. Define regulons from associations
    - association threshold
    - association DPI

"""

import os
import pandas as pd

# variables
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
RAW_DIR = os.path.join(ROOT,"data","raw")
PREP_DIR = os.path.join(ROOT,"data","prep")
SUPPORT_DIR = os.path.join(ROOT,"support")
RESULTS_DIR = os.path.join(ROOT,"results","regulon_inference")
SAVE_PARAMS = {"sep":"\t", "index":False, "compression":"gzip"}

CELL_LINES = ["K562","HepG2"]
DPSI_TYPES = ["delta_psi","delta_psi_rel"]

CANCER_TYPES = [
#     'ACC',
#     'BLCA',
#     'BRCA',
#     'CESC',
#     'CHOL',
#     'COAD',
#     'DLBC',
#     #'ESCA',
#     'GBM',
#     'HNSC',
#     'KICH',
#     'KIRC',
#     'KIRP',
#     #'LAML',
#     'LGG',
    'LIHC',
#     'LUAD',
#     'LUSC',
#     'MESO',
#     #'OV',
#     'PAAD',
#     'PCPG',
#     'PRAD',
#     'READ',
#     'SARC',
#     'SKCM',
#     #'STAD',
#     'TGCT',
#     'THCA',
#     'THYM',
#     'UCEC',
#     'UCS',
#     'UVM'
]
DATASETS = CANCER_TYPES + ["CCLE"]

TARGETS_FILES = {
    "event_psi_imputed": os.path.join(PREP_DIR,"event_psi_imputed","{dataset}-EX.tsv.gz"),
    "event_psi": os.path.join(PREP_DIR,"event_psi","{dataset}-EX.tsv.gz"),
    "genexpr_tpm": os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz")
}

OMICS = ["genexpr_vs_psi","genexpr_vs_psi_imputed","genexpr_vs_genexpr"]
METHODS_ASSOC = ["aracne","correlation_spearman","linear_model"]

SPLICING_FACTORS = pd.read_table(os.path.join(SUPPORT_DIR,"20230324-splicing_factors.tsv"))
SPLICING_FACTORS = SPLICING_FACTORS.loc[SPLICING_FACTORS["in_postar3"] & SPLICING_FACTORS["in_encore_kd"]]
SPLICING_FACTORS = SPLICING_FACTORS["ENSEMBL"].to_list()

N_SAMPLES = {"CCLE": 1015, "LIHC": 423}

N_BOOTSTRAPS = 10
BOOTSTRAPS = list(range(N_BOOTSTRAPS))
##### RULES #####
rule all:
    input:
        # regulons POSTAR3
        os.path.join(RESULTS_DIR,"files","regulons","CLIP","POSTAR3.tsv.gz"),
        
        # regulons ENCORE KDs
        expand(os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCORE","{cell_line}","{dpsi_type}.tsv.gz"), 
               cell_line=CELL_LINES, 
               dpsi_type=["delta_psi-EX","delta_psi-EX-masked","delta_psi_rel-EX","delta_psi_rel-EX-masked"]),
        
        # merge perturbations RNA seq
        expand(os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dpsi_type}-merged.tsv.gz"), dpsi_type=["delta_psi-EX","delta_psi-EX-masked","delta_psi_rel-EX","delta_psi_rel-EX-masked"]),
        
        # associations: mutual information (ARACNe), spearman correlation, linear model
        ## prepare inputs
        expand(os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors.tsv"), dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","inputs","{dataset}","names-splicing_factors.txt"), dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi-exons.tsv"), dataset=DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi_imputed-exons.tsv"), dataset=DATASETS),
        
        ## split regulators
        expand(os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors-{sf}.tsv"), dataset=DATASETS, sf=SPLICING_FACTORS),

        ## compute associations for each splicing factor
        ### SF vs exon
        expand(
            os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}.tsv.gz").format(
                dataset="{dataset}", omic="genexpr_vs_psi_imputed", method="{method}", sf="{sf}"
            ),
            dataset=DATASETS, method=METHODS_ASSOC+["linear_model2"], sf=SPLICING_FACTORS
        ),
        ### SF vs SF
        expand(
            os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}.tsv.gz").format(
                dataset="{dataset}", omic="genexpr_vs_genexpr", method="{method}"
            ),
            dataset=DATASETS, method=METHODS_ASSOC
        )
        
        ### mutual information - with ARACNe
        #         #### bootstraps
        #         expand(
        #             os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}",".done_bootstrap_{boot_i}").format(
        #                 omic="genexpr_vs_psi_imputed", method="aracne", dataset="{dataset}", sf="{sf}"
        #             ), dataset=DATASETS, sf=SPLICING_FACTORS
        #         ),
        #         #### consolidate bootstraps
        #         expand(
        #             os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}","network.txt").format(
        #                 omic="genexpr_vs_psi_imputed", method="aracne", dataset="{dataset}", sf="{sf}"
        #             ), dataset=DATASETS, sf=SPLICING_FACTORS
        #         ),
        #         #### prepare outputs
        #         expand(
        #             os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}.tsv.gz")
        #         )
        
        ### spearman correlation
        ### linear model
        
        ## compute associations between splicing factors
        ### ARACNe - mutual information
        ## filter associations by significance threshold 
        ## merge significant associations (regulon format)
        ## prune associations with DPI (regulon format)
        
        
        #         # run ARACNe
        #         ## prep inputs aracne
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","upstream_regs.tsv"), omic=OMICS, dataset=DATASETS),
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","upstream_regulators.txt"), omic=OMICS, dataset=DATASETS),
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","targets.tsv"), omic=OMICS, dataset=DATASETS),
        #         ## infer threshold
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}",".done_threshold"), omic=OMICS, dataset=DATASETS),
        #         ## bootstrap
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}"), omic=OMICS, dataset=DATASETS, boot_i=BOOTSTRAPS),
        #         ## consolidate
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","network.txt"), omic=OMICS, dataset=DATASETS),
        #         ## prep outputs for viper
        #         expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","regulons.tsv.gz"), omic=OMICS, dataset=DATASETS),


        #         # target inference
        #         ## correlations
        #         #expand(os.path.join(RESULTS_DIR,"files","target_inference","{method}","{omic}","{dataset}.tsv.gz"), method=["correlation_spearman"], dataset=DATASETS, omic=OMICS),
        #         ## aracne
        #         #expand(os.path.join(RESULTS_DIR,"files","target_inference","aracne_py","{omic}","{dataset}.tsv.gz"), dataset=DATASETS, omic=OMICS),

        

rule regulons_clip:
    input:
        mapped_peaks = os.path.join(PREP_DIR,"clip_peaks_mapped","POSTAR3.tsv.gz"),
        regulators = os.path.join(SUPPORT_DIR,"20230324-splicing_factors.tsv")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","CLIP","POSTAR3.tsv.gz")
    run:
        import pandas as pd
        
        # load
        mapped_peaks = pd.read_table(input.mapped_peaks)
        regulators = pd.read_table(input.regulators)
        
        # prep regulators
        regulators = regulators[["GENE","ENSEMBL"]]
        
        # get splicing factor
        mapped_peaks = mapped_peaks.loc[mapped_peaks["RBP"].isin(regulators["GENE"])].copy()
        
        # keep only exon-sf relationships
        mapped_peaks = mapped_peaks[["EVENT","RBP"]].drop_duplicates().copy()
        
        # add ensembl
        mapped_peaks = pd.merge(
            mapped_peaks, regulators.rename(columns={"GENE":"RBP"}), 
            on="RBP", how="left"
        )
        
        # format
        mapped_peaks["regulator"] = mapped_peaks["ENSEMBL"]
        mapped_peaks["target"] = mapped_peaks["EVENT"]
        mapped_peaks["likelihood"] = 1
        mapped_peaks["tfmode"] = 1
        
        # save
        mapped_peaks.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule regulons_pert_rnaseq:
    input:
        perts = os.path.join(PREP_DIR,'ground_truth_kd','ENCORE',"{cell_line}",'{dpsi_type}.tsv.gz'),
        regulators = os.path.join(SUPPORT_DIR,"20230324-splicing_factors.tsv")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCORE","{cell_line}","{dpsi_type}.tsv.gz")
    params:
        dpsi_type = "{dpsi_type}"
    run:
        import pandas as pd
        import numpy as np
        
        # load
        perts = pd.read_table(input.perts, index_col=0)
        regulators = pd.read_table(input.regulators)
        dpsi_type = params.dpsi_type
        
        # subset
        common_regulators = set(perts.columns).intersection(regulators["ENSEMBL"])
        perts = perts[common_regulators].copy()
        
        # prep regulators
        regulators = regulators[["GENE","ENSEMBL"]]
        
        # prep perturbations
        perts.index.name = "EVENT"
        perts = perts.melt(
            ignore_index=False, var_name="ENSEMBL", value_name=dpsi_type
        ).dropna().reset_index().copy()
        
        # add gene symbols
        perts = pd.merge(perts, regulators, on="ENSEMBL", how="left")
        
        # format
        perts["regulator"] = perts["ENSEMBL"]
        perts["target"] = perts["EVENT"]
        perts["likelihood"] = np.abs(perts[dpsi_type])
        perts["tfmode"] = np.sign(perts[dpsi_type])
        
        # save
        perts.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule combine_regulons_pert_rnaseq:
    input:
        [os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCORE",cell_line,"{dpsi_type}.tsv.gz")
         for cell_line in CELL_LINES]
    output:
        os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dpsi_type}-merged.tsv.gz")
    run:
        import pandas as pd
        import os
        
        perts = []
        for f in input:
            dataset = os.path.basename(os.path.dirname(os.path.dirname(f)))
            cell_line = os.path.basename(os.path.dirname(f))
            
            df = pd.read_table(f)
            df["cell_line"] = cell_line
            df["dataset"] = dataset
            
            perts.append(df)
            
        perts = pd.concat(perts)
        
        # save
        perts.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
            

rule prep_inputs_regulon_inference:
    input:
        regulators = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors-ensembl.txt"),
        targets = os.path.join(PREP_DIR,"event_psi","{dataset}-EX.tsv.gz"),
        targets_imputed = os.path.join(PREP_DIR,"event_psi_imputed","{dataset}-EX.tsv.gz")
    output:
        regulators = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","names-splicing_factors.txt"),
        targets = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi-exons.tsv"),
        targets_imputed = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi_imputed-exons.tsv")
    params:
        dataset = "{dataset}"
    threads: 1
    resources:
        runtime = 3600*1, # h (event PSI needs more time)
        memory = 20, #GB
    run:
        import pandas as pd
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = list(pd.read_table(input.regulators_oi, header=None)[0])
        targets = pd.read_table(input.targets, index_col=0)
        targets_imputed = pd.read_table(input.targets_imputed, index_col=0)
        dataset = params.dataset
        
        # indices
        regulators.index.name = "regulator"
        targets.index.name = "target"
        targets_imputed.index.name = "target"
        
        # subset
        common_regulators = set(regulators_oi).intersection(regulators.index)
        regulators = regulators.loc[common_regulators].copy()
        
        # check order
        common_samples = set(targets.columns).intersection(regulators.columns)
        targets = targets[common_samples].copy()
        targets_imputed = targets_imputed[common_samples].copy()
        regulators = regulators[common_samples].copy()

        # drop events and genes with little variation
        to_keep = (targets.quantile(0.75, axis=1) - targets.quantile(0.25, axis=1)) >= 1
        targets = targets.loc[to_keep]
        
        to_keep = (targets_imputed.quantile(0.75, axis=1) - targets_imputed.quantile(0.25, axis=1)) >= 1
        targets_imputed = targets_imputed.loc[to_keep]
        
        #to_keep = (regulators.quantile(0.75, axis=1) - regulators.quantile(0.25, axis=1)) >= 1
        #regulators = regulators.loc[to_keep]
        
        # subset targets
        common_targets = set(targets.index).intersection(targets_imputed.index)
        targets = targets.loc[common_targets].copy()
        targets_imputed = targets_imputed.loc[common_targets].copy()
        
        # save
        regulators.reset_index().to_csv(output.regulators, sep="\t", index=None)
        pd.DataFrame(regulators.index).to_csv(
            output.regulators_oi, sep="\t", index=None, header=False
        )
        targets.reset_index().to_csv(output.targets, sep="\t", index=None)
        targets_imputed.reset_index().to_csv(output.targets_imputed, sep="\t", index=None)
        
        print("Done!")

        
rule split_regulators:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors.tsv"),
    output:
        regulator = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors-{sf}.tsv")
    params:
        sf = "{sf}"
    run:
        import pandas as pd
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        sf = params.sf

        # save
        regulators.loc[[sf]].reset_index().to_csv(output.regulator, sep="\t", index=None)
        
        print("Done!")

        
rule associations_sf_exon_lm2:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors-{sf}.tsv"),
        targets_splicing = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi_imputed-exons.tsv"),
        targets_genexpr = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        mapping = os.path.join(RAW_DIR,"VastDB","event_annotation-Hs2.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}.tsv.gz")
    params:
        method = "{method}"
    threads: 6
    resources:
        runtime = int(3600*0.5), # h 
        memory = 6,
    shell:
        """
        if [[ "{params.method}" == "linear_model2" ]]; then

            python scripts/compute_associations.py \
                        --regulators_file={input.regulators} \
                        --targets_file={input.targets_splicing} \
                        --targets_genexpr_file={input.targets_genexpr} \
                        --mapping_file={input.mapping} \
                        --method={params.method} \
                        --n_jobs={threads} \
                        --output_file={output}
        else
            python scripts/compute_associations.py \
                        --regulators_file={input.regulators} \
                        --targets_file={input.targets_splicing} \
                        --method={params.method} \
                        --n_jobs={threads} \
                        --output_file={output}
        fi
        """
    
    
rule associations_sf_sf:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors.tsv"),
        targets = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors.tsv")
    output:
        os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}.tsv.gz")
    params:
        method = "{method}"
    threads: 12
    resources:
        runtime = int(3600*0.5), # h 
        memory = 6,
    shell:
        """
        python scripts/compute_associations.py \
                    --regulators_file={input.regulators} \
                    --targets_file={input.targets} \
                    --method={params.method} \
                    --n_jobs={threads} \
                    --output_file={output}
        """

################
rule mutual_information_genexpr_psi_bootstrap:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","genexpr_tpm-splicing_factors-{sf}.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","names-splicing_factors.txt"),
        targets = os.path.join(RESULTS_DIR,"files","inputs","{dataset}","event_psi_imputed-exons.tsv")
    output:
        touch(os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}",".done_bootstrap_{boot_i}"))
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}"),
        random_seed = "{boot_i}",
        n_samples = lambda wildcards: N_SAMPLES[wildcards.dataset]
    threads: 6
    resources:
        runtime = int(3600*0.5), # h (event PSI needs more time)
        memory = 6,
    shell:
        """
        set -eo pipefail
        
        # make directory
        mkdir -p {params.output_dir}
        
        # hack p-value threshold
        echo 0.0 > {params.output_dir}/miThreshold_p1E-8_samples{params.n_samples}.txt
        
        # compute mutual informations
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --expfile_upstream {input.regulators} \
                --tfs {input.regulators_oi} \
                --expfile_downstream {input.targets} \
                --output {params.output_dir} \
                --pvalue 1E-8 \
                --seed {params.random_seed} \
                --nodpi \
                --threads {threads}
                
        echo "Done!"
        """

        
rule mutual_information_genexpr_psi_consolidation:
    input:
        [os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}",".done_bootstrap_{boot_i}").format(
            dataset="{dataset}", omic="{omic}", method="{method}", sf="{sf}", boot_i=boot_i) for boot_i in BOOTSTRAPS]
    output:
        os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}","network.txt.gz")
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}"),
        consolidate_pvalue = 1.1
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 5,
    shell:
        """
        # consolidate
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --output {params.output_dir} \
                --threads {threads} \
                --consolidatepvalue {params.consolidate_pvalue} \
                --nobonferroni \
                --consolidate
                
        # compress
        gzip {params.output_dir}/network.txt
                
        echo "Done!"
        """
        
        
rule mutual_information_prep_outputs:
    input:
        assocs = os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}","network.txt")
    output:
        assocs = os.path.join(RESULTS_DIR,"files","associations","{dataset}","{omic}","{method}","{sf}.tsv.gz")
    params:
        method = "{method}"
    run:
        import pandas as pd
        
        # load
        assocs = pd.read_table(input.assocs)
        method = params.method
        
        # rename
        assocs = assocs.rename(columns={"Regulator":"regulator", "Target":"target", "MI":"association"})
        assocs["method"] = method
        
        # save
        assocs.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
# rule target_inference_aracne_java_threshold:
#     input:
#         regulators = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","upstream_regs.tsv"),
#         regulators_oi = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","upstream_regulators.txt"),
#         targets = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","targets.tsv")
#     output:
#         touch(os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}",".done_threshold")),
#     params:
#         aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
#         random_seed = 1,
#         output_dir = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}"),
#         mi_pvalue_thresh = "1E-8"
#     threads: 12
#     resources:
#         runtime = 3600*12, # h (event PSI needs more time)
#         memory = 15,
#     shell:
#         """
#         java -Xmx{resources.memory}G -jar {params.aracne_bin} \
#                 --expfile_upstream {input.regulators} \
#                 --tfs {input.regulators_oi} \
#                 --expfile_downstream {input.targets} \
#                 --output {params.output_dir} \
#                 --pvalue {params.mi_pvalue_thresh} \
#                 --seed {params.random_seed} \
#                 --threads {threads} \
#                 --calculateThreshold
#         """
        

rule target_inference_aracne_java_prep_output:
    input:
        regulons = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","network.txt"),
        regulators = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        targets = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","targets.tsv")
    output:
        regulons = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","regulons.tsv.gz"),
        interactions_regulators = os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}","interactions_regulators.tsv.gz")
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 15, #GB
    params:
        padj_method = "fdr_bh",
        padj_threshold = 0.05
    run:
        import pandas as pd
        from statsmodels.stats.multitest import multipletests
        from joblib import Parallel, delayed
        from tqdm import tqdm
        
        # load
        regulons = pd.read_table(input.regulons)
        regulators = pd.read_table(input.regulators, index_col=0)
        targets = pd.read_table(input.targets, index_col=0)
        padj_method = params.padj_method
        padj_threshold = params.padj_threshold
        n_jobs = threads
        
        # split regulator-target interactions from regulator-regulator interactions
        is_reg_reg = regulons["Target"].isin(regulators.index)
        interactions_regulators = regulons.loc[is_reg_reg].copy()
        regulons = regulons.loc[~is_reg_reg].copy()
        
        # prepare regulon
        ## multiple test correction
        _, regulons["padj"], _, _ = multipletests(
            regulons["pvalue"], method=padj_method
        )
        ## filter out non-significant regulator-targets
        regulons = regulons.loc[regulons["padj"] < padj_threshold].copy()
        
        ## "upstream_regulator" and "target" columns
        regulons["regulator"] = regulons["Regulator"]
        regulons["target"] = regulons["Target"]
        
        ## likelihood
        regulons["likelihood"] = regulons["MI"]
        
        # compute spearman correlation for each pair
        def correlate(reg, tar, reg_name, tar_name, method="spearman"):
            corr = reg.corr(tar, method=method)
            corr = pd.Series({"regulator": reg_name, "target": tar_name, "tfmode": corr})
            return corr
        
        tfmodes = Parallel(n_jobs=n_jobs)(
            delayed(correlate)(
                regulators.loc[regulator],
                targets.loc[target],
                regulator,
                target
            ) for regulator, target in tqdm(regulons[["regulator","target"]].values)
        )
        tfmodes = pd.DataFrame(tfmodes)
        regulons = pd.merge(regulons, tfmodes, on=["regulator","target"], how="left")
        
        #         regulons["tfmode"] = regulons.apply(
        #             lambda row: regulators.loc[row["regulator"]].corr(targets.loc[row["target"]], method="spearman"), axis=1
        #         )

        # save
        regulons.to_csv(output.regulons, sep="\t", index=None, compression="gzip")
        interactions_regulators.to_csv(output.interactions_regulators, sep="\t", index=None, compression="gzip")
        
        print("Done!")



# rule target_inference_aracne_py:
#     input:
#         targets = lambda wildcards: TARGETS_FILES[wildcards.omic] if wildcards.omic!="event_psi" else TARGETS_FILES[wildcards.omic+"_imputed"],
#         upstream_regs = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
#         upstream_regs_oi = os.path.join(SUPPORT_DIR,"GOBP_RNA_SPLICING-ensembl.txt"),
#     output:
#         os.path.join(RESULTS_DIR,"files","target_inference","aracne_py","{omic}","{dataset}.tsv.gz")
#     params:
#         method="aracne_py"
#     threads: 24
#     resources:
#         runtime = 3600*12, # h (event PSI needs more time)
#         memory = 15
#     shell:
#         """
#         nice python scripts/infer_targets.py \
#                     --targets_file={input.targets} \
#                     --upstream_regs_file={input.upstream_regs} \
#                     --upstream_regs_oi_file={input.upstream_regs_oi} \
#                     --method={params.method} \
#                     --n_jobs={threads} \
#                     --output_file={output}
#         """

        

#ruleorder: target_inference_aracne_py > target_inference
        
# rule target_inference:
#     input:
#         targets = lambda wildcards: TARGETS_FILES[wildcards.omic],
#         upstream_regs = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
#         upstream_regs_oi = os.path.join(SUPPORT_DIR,"GOBP_RNA_SPLICING-ensembl.txt")
#     output:
#         os.path.join(RESULTS_DIR,"files","target_inference","{method}","{omic}","{dataset}.tsv.gz")
#     params:
#         method="{method}"
#     threads: 12
#     shell:
#         """
#         nice python scripts/infer_targets.py \
#                     --targets_file={input.targets} \
#                     --upstream_regs_file={input.upstream_regs} \
#                     --upstream_regs_oi_file={input.upstream_regs_oi} \
#                     --method={params.method} \
#                     --n_jobs={threads} \
#                     --output_file={output}
#         """
        
        
# rule target_inference_aracne_java:
#     input:
#         targets = lambda wildcards: TARGETS_FILES[wildcards.omic] if wildcards.omic!="event_psi" else TARGETS_FILES[wildcards.omic+"_imputed"],
#         upstream_regs = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
#         upstream_regs_oi = os.path.join(SUPPORT_DIR,"GOBP_RNA_SPLICING-ensembl.txt"),
#         spearman = os.path.join(RESULTS_DIR,"files","target_inference","correlation_spearman","{omic}","{dataset}.tsv.gz")
#     output:
#         os.path.join(RESULTS_DIR,"files","target_inference","aracne","{omic}","{dataset}.tsv.gz")
#     params:
#         method="aracne"
#     threads: 24
#     resources:
#         runtime = 3600*12, # h (event PSI needs more time)
#         memory = 15
#     shell:
#         """
#         nice python scripts/infer_targets.py \
#                     --targets_file={input.targets} \
#                     --upstream_regs_file={input.upstream_regs} \
#                     --upstream_regs_oi_file={input.upstream_regs_oi} \
#                     --correlation_spearman_file={input.spearman} \
#                     --method={params.method} \
#                     --n_jobs={threads} \
#                     --output_file={output}
#         """
