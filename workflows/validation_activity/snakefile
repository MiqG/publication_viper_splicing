"""
Author: Miquel Anglada Girotto
Contact: miquelangladagirotto [at] gmail [dot] com
"""

import os
# variables
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
RAW_DIR = os.path.join(ROOT,"data","raw")
PREP_DIR = os.path.join(ROOT,"data","prep")
SUPPORT_DIR = os.path.join(ROOT,"support")
RESULTS_DIR = os.path.join(ROOT,"results","validation_activity")
SAVE_PARAMS = {"sep":"\t", "index":False, "compression":"gzip"}

THRESHS_DPSI = {
    "morethan": [0,5,10,15,20,25,30,35],
    "lessthan": [  5,10,15,20,25,30,35,101]
}
THRESH_TYPES = ["lessthan","morethan"]

CELL_LINES = ["K562","HepG2"]
ENCORE_DATASETS = ["ENCOREKD","ENCOREKO"]
REGULONS = {
    "ENCOREKD_HepG2": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKD","HepG2","delta_psi-EX.tsv.gz"),
    "ENCOREKD_K562": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKD","K562","delta_psi-EX.tsv.gz"),
    "ENCOREKO_HepG2": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKO","HepG2","delta_psi-EX.tsv.gz"),
    "ENCOREKO_K562": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKO","K562","delta_psi-EX.tsv.gz")
}

# ARACNe
TARGETS_FILES = {
    #"event_psi_imputed": os.path.join(PREP_DIR,"event_psi_imputed","{dataset}-EX.tsv.gz"),
    #"event_psi": os.path.join(PREP_DIR,"event_psi","{dataset}-EX.tsv.gz"),
    "genexpr_tpm": os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz")
}

ARACNE_OMICS = ["genexpr_tpm"]
ARACNE_DATASETS = ["LIHC","LAML","CCLE"]
N_BOOTSTRAPS = 100
BOOTSTRAPS = list(range(N_BOOTSTRAPS))

##### RULES #####
rule all:
    input:        
        # make regulons
        ## from perturbation RNA seqs
        expand(os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dataset}","{cell_line}","delta_psi-EX.tsv.gz"), dataset=ENCORE_DATASETS, cell_line=CELL_LINES),
        ## from ARACNe genexpr vs genexpr
        ### prep inputs aracne
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### infer threshold
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### bootstrap
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS, boot_i=BOOTSTRAPS),
        ### consolidate
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### compute spearman correlations (tfmode)
        expand(os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### prep outputs for viper
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","regulons.tsv.gz"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        
        # evaluate ENCORE KD ground truth
        ## subset regulons with different dPSI thresholds
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_lessthan_{thresh}.tsv.gz"), dataset=REGULONS.keys(), thresh=THRESHS_DPSI["lessthan"]),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_{thresh}.tsv.gz"), dataset=REGULONS.keys(), thresh=THRESHS_DPSI["morethan"]),

        ## infer protein activities with those networks
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz"), dataset_regulon=REGULONS.keys(), dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES, thresh=THRESHS_DPSI["lessthan"], thresh_type="lessthan"),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz"), dataset_regulon=REGULONS.keys(), dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES, thresh=THRESHS_DPSI["morethan"], thresh_type="morethan"),

        ## combine inferred protein activities
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged.tsv.gz"),

        ## evaluate inferred protein activities
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings.tsv.gz"),
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs.tsv.gz"),

        # save selected regulons
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons_selected","{dataset}-dpsi_morethan_15.tsv.gz"), dataset=REGULONS.keys()),
        
        # figures evaluation networks
        os.path.join(RESULTS_DIR,'figures','evaluation')
        
        
rule regulons_pert_rnaseq:
    input:
        perts = os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'delta_psi-EX.tsv.gz'),
        regulators = os.path.join(SUPPORT_DIR,"splicing_factors.tsv")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dataset}","{cell_line}","delta_psi-EX.tsv.gz")
    params:
        dpsi_type = "delta_psi-EX"
    run:
        import pandas as pd
        import numpy as np
        
        # load
        perts = pd.read_table(input.perts, index_col=0)
        regulators = pd.read_table(input.regulators)
        dpsi_type = params.dpsi_type
        
        # subset
        idx = regulators["source"].isin(["Rogalska2022","Hegele2012","hand-curated"])
        regulators = regulators.loc[idx]
        common_regulators = set(perts.columns).intersection(regulators["ENSEMBL"])
        perts = perts[common_regulators].copy()
        
        # prep regulators
        regulators = regulators[["GENE","ENSEMBL"]]
        
        # prep perturbations
        perts.index.name = "EVENT"
        perts = perts.melt(
            ignore_index=False, var_name="ENSEMBL", value_name=dpsi_type
        ).dropna().reset_index().copy()
        
        # add gene symbols
        perts = pd.merge(perts, regulators, on="ENSEMBL", how="left")
        
        # format
        perts["regulator"] = perts["ENSEMBL"]
        perts["target"] = perts["EVENT"]
        perts["likelihood"] = np.abs(perts[dpsi_type])
        perts["tfmode"] = (-1)*np.sign(perts[dpsi_type]) # they come from KD or KO, decrease activity
        
        # save
        perts.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule target_inference_arache_java_prep_inputs:
    input:
        regulators = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors-ensembl.txt"),
        targets = lambda wildcards: TARGETS_FILES[wildcards.omic] if wildcards.omic!="event_psi" else TARGETS_FILES[wildcards.omic+"_imputed"],
    output:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    threads: 1
    resources:
        runtime = 3600*1, # h
        memory = 5,
    run:
        import pandas as pd
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = list(pd.read_table(input.regulators_oi, header=None)[0])
        targets = pd.read_table(input.regulators, index_col=0)
        
        # indices
        regulators.index.name = "regulator"
        targets.index.name = "target"
        
        # subset
        common_regulators = set(regulators_oi).intersection(regulators.index)
        regulators = regulators.loc[common_regulators].copy()
        
        # check order
        common_samples = set(targets.columns).intersection(regulators.columns)
        targets = targets[common_samples].copy()
        regulators = regulators[common_samples].copy()

        # drop events and genes with no variation
        targets = targets.loc[targets.std(1) > 0]
        regulators = regulators.loc[regulators.std(1) > 0]
        
        # save
        regulators.reset_index().to_csv(output.regulators, sep="\t", index=None)
        pd.DataFrame(regulators.index).to_csv(
            output.regulators_oi, sep="\t", index=None, header=False
        )
        targets.reset_index().to_csv(output.targets, sep="\t", index=None)
        
        print("Done!")
        

rule target_inference_aracne_java_threshold:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    output:
        touch(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold")),
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        random_seed = 1,
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        mi_pvalue_thresh = "1E-8"
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 5,
    shell:
        """
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --expfile_upstream {input.regulators} \
                --tfs {input.regulators_oi} \
                --expfile_downstream {input.targets} \
                --output {params.output_dir} \
                --pvalue {params.mi_pvalue_thresh} \
                --seed {params.random_seed} \
                --threads {threads} \
                --calculateThreshold
        """
        
        
rule target_inference_aracne_java_bootstrap:
    input:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"),
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    output:
        touch(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}"))
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        random_seed = "{boot_i}"
    threads: 6
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 5,
    shell:
        """
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --expfile_upstream {input.regulators} \
                --tfs {input.regulators_oi} \
                --expfile_downstream {input.targets} \
                --output {params.output_dir} \
                --pvalue 1E-8 \
                --seed {params.random_seed} \
                --threads {threads}
        """
        
        
rule target_inference_aracne_java_consolidation:
    input:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"),
        [os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}").format(boot_i=boot_i, omic="{omic}", dataset="{dataset}") for boot_i in BOOTSTRAPS]
    output:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt")
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        consolidate_pvalue = 0.05
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 15,
    shell:
        """
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --output {params.output_dir} \
                --threads {threads} \
                --consolidatepvalue {params.consolidate_pvalue} \
                --consolidate
        """
        
        
rule target_inference_aracne_spearman_correlation:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")        
    output:
        os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz")
    threads: 12
    run:
        from scipy import stats
        from statsmodels.stats import multitest
        import numpy as np
        import pandas as pd
        from joblib import Parallel, delayed
        from tqdm import tqdm

        def corr(a, b, method):
            idx = np.isfinite(a) & np.isfinite(b)
            a = a[idx]
            b = b[idx]

            corr_funcs = {
                "correlation_spearman": stats.spearmanr,
                "correlation_pearson": stats.pearsonr,
            }
            corr_func = corr_funcs[method]

            try:
                statistic, pvalue = corr_func(a, b)
                result = pd.Series(
                    {
                        "statistic": statistic,
                        "pvalue": pvalue,
                        "n_samples": len(a),
                        "method": method,
                    }
                )
            except:
                result = pd.Series(
                    {
                        "statistic": np.nan,
                        "pvalue": np.nan,
                        "n_samples": np.nan,
                        "method": method,
                    }
                )

            return result


        def compute_correlation_single(targets, upstream_reg_single, method):
            correl = targets.apply(
                lambda x: corr(x, upstream_reg_single, method), axis=1
            ).dropna()
            correl["regulator"] = upstream_reg_single.name
            correl = correl.reset_index()

            # prepare regulon
            ## likelihood
            correl["likelihood"] = np.abs(correl["statistic"])
            ## tfmode
            correl["tfmode"] = correl["statistic"]
            ## keep only significant correlations
            correl["padj"] = np.nan
            _, fdr, _, _ = multitest.multipletests(
                correl.loc[np.isfinite(correl["pvalue"]), "pvalue"], method="fdr_bh"
            )
            correl.loc[np.isfinite(correl["pvalue"]), "padj"] = fdr
            ## filter out irrelevant associations
            #correl = correl.loc[correl["padj"] < THRESH_FDR].copy()

            return correl


        def compute_correlations(targets, regulators, n_jobs, method):
            correls = Parallel(n_jobs=n_jobs)(
                delayed(compute_correlation_single)(targets, regulators.loc[reg_oi], method)
                for reg_oi in tqdm(regulators.index)
            )
            result = pd.concat(correls)
            result = result.reset_index()

            return result        
        
        # load data
        ## load
        targets = pd.read_table(input.targets, index_col=0)
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = list(pd.read_table(input.regulators_oi, header=None)[0])
        n_jobs = threads
        ## index
        targets.index.name = "target"
        regulators.index.name = "regulator"
        ## subset
        genes_oi = set(regulators_oi).intersection(regulators.index)
        regulators = regulators.loc[genes_oi].copy()
        ## check order
        common_samples = set(targets.columns).intersection(regulators.columns)
        targets = targets[common_samples].copy()
        regulators = regulators[common_samples].copy()        
        
        print(targets.shape, regulators.shape)
        
        # compute correlations
        result = compute_correlations(
            targets, regulators, n_jobs, "correlation_spearman"
        )
        
        # save
        print("Saving data...")
        result.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
    
rule target_inference_aracne_java_prep_output:
    input:
        regulons = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt"),
        spearman = os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz")
    output:
        regulons = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","regulons.tsv.gz")
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 15,
    run:
        import pandas as pd
        
        # load
        regulons = pd.read_table(input.regulons)
        spearman = pd.read_table(input.spearman)
        
        
        # prepare regulon
        ## "regulator" and "target" columns
        regulons["regulator"] = regulons["Regulator"]
        regulons["target"] = regulons["Target"]
        
        ## likelihood
        regulons["likelihood"] = regulons["MI"]
        
        ## tfmode
        regulons = pd.merge(
            regulons,
            spearman[["regulator", "target", "tfmode"]],
            on=["regulator", "target"],
            how="left",
        )
        
        regulons.loc[regulons["tfmode"].isnull(), "tfmode"] = 0
        
        # save
        regulons.reset_index().to_csv(output.regulons, sep="\t", index=None, compression="gzip")

        print("Done!")
        

rule subset_regulons_lessthan:
    input:
        regulons = lambda wildcards: REGULONS[wildcards.dataset]
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_lessthan_{thresh}.tsv.gz")
    params:
        thresh = "{thresh}"
    run:
        import pandas as pd
        
        regulons = pd.read_table(input.regulons)
        thresh = float(params.thresh)
        
        regulons = regulons.loc[regulons["likelihood"] <= thresh]
        
        regulons.to_csv(output.regulons, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule subset_regulons_morethan:
    input:
        regulons = lambda wildcards: REGULONS[wildcards.dataset]
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_{thresh}.tsv.gz")
    params:
        thresh = "{thresh}"
    run:
        import pandas as pd
        
        regulons = pd.read_table(input.regulons)
        thresh = float(params.thresh)
        
        regulons = regulons.loc[regulons["likelihood"] >= thresh]
        
        regulons.to_csv(output.regulons, **SAVE_PARAMS)
        
        print("Done!")

        
rule compute_protein_activity:
    input:
        signature = os.path.join(PREP_DIR,"ground_truth_pert","{dataset_signature}","{cell_line}","delta_psi-EX.tsv.gz"),
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset_regulon}","dpsi_{thresh_type}_{thresh}.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz")
    params:
        assoc_method = "ground_truth",
        actinf_method = "viper"
    shell:
        """
        Rscript scripts/infer_protein_activity.R \
                    --signature_file={input.signature} \
                    --regulons_file={input.regulons} \
                    --output_file={output} \
                    --assoc_method={params.assoc_method} \
                    --actinf_method={params.actinf_method}
        """

        
rule combine_inferred_protein_activities:
    input:
        protein_activities = [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line, thresh_type="lessthan", thresh=thresh) for
            dataset_regulon in REGULONS.keys() for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES for thresh in THRESHS_DPSI["lessthan"]
        ] + [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line, thresh_type="morethan", thresh=thresh) for
            dataset_regulon in REGULONS.keys() for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES for thresh in THRESHS_DPSI["morethan"]
        ]
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged.tsv.gz")
    run:
        import os
        import pandas as pd
        
        dfs = []
        for file in input.protein_activities:
            dataset_regulon = os.path.basename(os.path.dirname(file))
            dataset_signature = os.path.basename(file).split("-")[0]
            cell_line = os.path.basename(file).split("-")[1]
            thresh_type = os.path.basename(file).split("-")[2].replace(".tsv.gz","").split("_")[1]
            thresh = os.path.basename(file).split("-")[2].replace(".tsv.gz","").split("_")[2]
            
            df = pd.read_table(file)
            df = df.melt(id_vars="regulator", var_name="PERT_GENE", value_name="protein_activity")
            
            df["dataset_regulon"] = dataset_regulon
            df["dataset_signature"] = dataset_signature
            df["cell_line"] = cell_line
            df["thresh_type"] = thresh_type
            df["thresh"] = float(thresh)
            
            dfs.append(df)
            
        dfs = pd.concat(dfs)
        
        dfs.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule evaluate_regulons:
    input:
        protein_activity = os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged.tsv.gz")
    output:
        evaluation_rankings = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings.tsv.gz"),
        evaluation_corrs = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs.tsv.gz")
    run:
        import pandas as pd
        from scipy import stats
        
        df = pd.read_table(input.protein_activity)
        
        vars_oi = ["thresh_type","thresh","dataset_regulon","dataset_signature","cell_line"]
        
        # ranking of perturbations across experiments
        df["ranking_between"] = df.groupby(vars_oi + ["regulator"])["protein_activity"].rank()
        df["rankperc_between"] = df["ranking_between"] / df.groupby(vars_oi + ["regulator"])["ranking_between"].transform('max')
        # ranking of perturbations within experiments
        df["ranking_within"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].rank()
        df["rankperc_within"] = df["ranking_within"] / df.groupby(vars_oi + ["PERT_GENE"])["ranking_within"].transform('max')
        df["eval_type"] = "real"
        # correlation between protein activities in both cell lines and regulons
        corrs = []
        for regulon in df["dataset_regulon"].unique():
            idx = (df["dataset_regulon"]==regulon) & (df["regulator"]==df["PERT_GENE"])
            X = df.loc[idx].pivot(
                index=["thresh_type","thresh","dataset_regulon","dataset_signature","regulator","PERT_GENE"],
                columns="cell_line",
                values="protein_activity"
            )
            pearson_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="pearson").iloc[0::2,-1]
            pearson_coef.name = "pearson_coef"
            spearman_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="spearman").iloc[0::2,-1]
            spearman_coef.name = "spearman_coef"
            corrs.append(
                pd.concat([pearson_coef, spearman_coef], axis=1).reset_index()
            )
        corrs = pd.concat(corrs)
        corrs["cell_line"] = "HepG2_vs_K562"
        corrs["eval_type"] = "real"
        
        # store
        evaluation_rankings = df.copy()
        evaluation_corrs = corrs.copy()

        # randomize
        df["protein_activity"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].sample(frac=1, random_state=1).values
        # ranking of perturbation across experiments
        df["ranking_between"] = df.groupby(vars_oi + ["regulator"])["protein_activity"].rank()
        df["rankperc_between"] = df["ranking_between"] / df.groupby(vars_oi + ["regulator"])["ranking_between"].transform('max')
        # ranking of perturbation within experiments
        df["ranking_within"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].rank()
        df["rankperc_within"] = df["ranking_within"] / df.groupby(vars_oi + ["PERT_GENE"])["ranking_within"].transform('max')
        # add ranking type
        df["eval_type"] = "random"
        # correlation between protein activities in both cell lines and regulons
        corrs = []
        for regulon in df["dataset_regulon"].unique():
            idx = (df["dataset_regulon"]==regulon) & (df["regulator"]==df["PERT_GENE"])
            X = df.loc[idx].pivot(
                index=["thresh_type","thresh","dataset_regulon","dataset_signature","regulator","PERT_GENE"],
                columns="cell_line",
                values="protein_activity"
            )
            pearson_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="pearson").iloc[0::2,-1]
            pearson_coef.name = "pearson_coef"
            spearman_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="spearman").iloc[0::2,-1]
            spearman_coef.name = "spearman_coef"
            corrs.append(
                pd.concat([pearson_coef, spearman_coef], axis=1).reset_index()
            )
        corrs = pd.concat(corrs)
        corrs["cell_line"] = "HepG2_vs_K562"
        corrs["eval_type"] = "random"
        
        # store
        evaluation_rankings = pd.concat([evaluation_rankings, df])
        evaluation_corrs = pd.concat([evaluation_corrs, corrs])
        
        # save
        evaluation_rankings.to_csv(output.evaluation_rankings, **SAVE_PARAMS)
        evaluation_corrs.to_csv(output.evaluation_corrs, **SAVE_PARAMS)
        
        print("Done!")

        
rule selected_regulons:
    input:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_15.tsv.gz")
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons_selected","{dataset}-dpsi_morethan_15.tsv.gz")
    shell:
        """
        set -eo pipefail
        
        cp {input.regulons} {output.regulons}
        
        echo "Done!"
        """


rule figures_validation_activity_encore:
    input:
        evaluation_rankings = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings.tsv.gz"),
        evaluation_corrs = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs.tsv.gz")
    output:
        directory(os.path.join(RESULTS_DIR,'figures','evaluation'))
    shell:
        """
        Rscript scripts/figures_evaluation_activity_encore.R \
                    --evaluation_rankings_file={input.evaluation_rankings} \
                    --evaluation_corrs_file={input.evaluation_corrs} \
                    --figs_dir={output}
        """