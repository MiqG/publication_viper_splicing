"""
Author: Miquel Anglada Girotto
Contact: miquelangladagirotto [at] gmail [dot] com
"""

import os
# variables
ROOT = os.path.dirname(os.path.dirname(os.getcwd()))
RAW_DIR = os.path.join(ROOT,"data","raw")
PREP_DIR = os.path.join(ROOT,"data","prep")
SUPPORT_DIR = os.path.join(ROOT,"support")
RESULTS_DIR = os.path.join(ROOT,"results","validation_activity")
SAVE_PARAMS = {"sep":"\t", "index":False, "compression":"gzip"}

EVENT_TYPE = ["EX","ALT5","ALT3","INT"] # TODO

THRESHS_DPSI = {
    "morethan": [0,5,10,15,20,25,30,35],
    "lessthan": [  5,10,15,20,25,30,35,101]
}
THRESH_TYPES = ["lessthan","morethan"]

CELL_LINES = ["K562","HepG2"]
ENCORE_DATASETS = ["ENCOREKD","ENCOREKO"]
REGULONS = {
    "ENCOREKD_HepG2": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKD","HepG2","regulons.tsv.gz"),
    "ENCOREKD_K562": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKD","K562","regulons.tsv.gz"),
    "ENCOREKO_HepG2": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKO","HepG2","regulons.tsv.gz"),
    "ENCOREKO_K562": os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","ENCOREKO","K562","regulons.tsv.gz")
}

# regulon inference from delta PSI based on splicing factor features
DPSI_REGINFS = ["diff_genexpr","diff_mutation","aracne"]

# ARACNe
TARGETS_FILES = {
    "event_psi_imputed": os.path.join(PREP_DIR,"event_psi_imputed","{dataset}-EX.tsv.gz"),
    "event_psi": os.path.join(PREP_DIR,"event_psi","{dataset}-EX.tsv.gz"),
    "genexpr_tpm": os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
    "discretized_qep": os.path.join(PREP_DIR,"event_psi_imputed_discretized_qep","{dataset}-EX.tsv.gz")
}

ARACNE_OMICS = ["genexpr_tpm","event_psi","discretized_qep"]
ARACNE_DATASETS = ["LIHC","LAML","CCLE"]
N_BOOTSTRAPS = 100
BOOTSTRAPS = list(range(N_BOOTSTRAPS))

# evaluation
OMIC_TYPES = ["fc_tpm","dpsi","dpsi_diff_genexpr","dpsi_diff_mutation","dpsi_aracne"]

##### RULES #####
rule all:
    input:        
        # make regulons
        ## from perturbation RNA seqs (ground truth)
        expand(os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dataset}","{cell_line}","regulons.tsv.gz"), dataset=ENCORE_DATASETS, cell_line=CELL_LINES),
        
        ## from differential splicing
        ### between samples with differentially expressed SFs
        expand(os.path.join(RESULTS_DIR,"files","regulons","diff_genexpr","{omic}","{dataset}","regulons.tsv.gz"), omic=["event_psi"], dataset=ARACNE_DATASETS),
        ### between samples with mutated SFs
        expand(os.path.join(RESULTS_DIR,"files","regulons","diff_mutation","{omic}","{dataset}","regulons.tsv.gz"), omic=["event_psi"], dataset=ARACNE_DATASETS),

        ## from ARACNe genexpr vs genexpr and genexpr vs PSI
        ### prep inputs aracne
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### infer threshold
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### bootstrap
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS, boot_i=BOOTSTRAPS),
        ### consolidate
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### compute spearman correlations (tfmode)
        expand(os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),
        ### prep outputs for viper
        expand(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","regulons.tsv.gz"), omic=ARACNE_OMICS, dataset=ARACNE_DATASETS),

        # evaluate ground truth
        ## subset regulons with different dPSI thresholds
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_lessthan_{thresh}.tsv.gz"), dataset=REGULONS.keys(), thresh=THRESHS_DPSI["lessthan"]),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_{thresh}.tsv.gz"), dataset=REGULONS.keys(), thresh=THRESHS_DPSI["morethan"]),

        ## infer protein activities with those networks
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz"), dataset_regulon=REGULONS.keys(), dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES, thresh=THRESHS_DPSI["lessthan"], thresh_type="lessthan"),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz"), dataset_regulon=REGULONS.keys(), dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES, thresh=THRESHS_DPSI["morethan"], thresh_type="morethan"),

        ## save selected regulons
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons_selected","{dataset}-dpsi_morethan_15.tsv.gz"), dataset=REGULONS.keys()),

        # compute protein activities delta PSI networks
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{diff_type}.tsv.gz"), dataset_regulon=ARACNE_DATASETS, dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES, diff_type=DPSI_REGINFS),

        # compute protein activities aracne genexpr networks
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-fc_tpm.tsv.gz"), dataset_regulon=ARACNE_DATASETS, dataset_signature=ENCORE_DATASETS, cell_line=CELL_LINES),

        # combine protein activities
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-{omic_type}.tsv.gz"), omic_type=OMIC_TYPES),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-dpsi_{diff_type}.tsv.gz"), diff_type=DPSI_REGINFS),

        # evaluate inferred protein activities        
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings-{omic_type}.tsv.gz"), omic_type=OMIC_TYPES),
        expand(os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs-{omic_type}.tsv.gz"), omic_type=OMIC_TYPES),

        # figures evaluation networks
        expand(os.path.join(RESULTS_DIR,'figures','evaluation-{omic_type}'), omic_type=OMIC_TYPES)
        
        
rule regulons_pert_rnaseq:
    input:
        perts = os.path.join(PREP_DIR,'ground_truth_pert','{dataset}',"{cell_line}",'delta_psi-EX.tsv.gz'),
        regulators = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors.tsv")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","pert_rnaseq","{dataset}","{cell_line}","regulons.tsv.gz")
    params:
        dpsi_type = "delta_psi-EX"
    run:
        import pandas as pd
        import numpy as np
        
        # load
        perts = pd.read_table(input.perts, index_col=0)
        regulators = pd.read_table(input.regulators)
        dpsi_type = params.dpsi_type
        
        # subset
        common_regulators = set(perts.columns).intersection(regulators["ENSEMBL"])
        perts = perts[common_regulators].copy()
        
        # prep regulators
        regulators = regulators[["GENE","ENSEMBL"]]
        
        # prep perturbations
        perts.index.name = "EVENT"
        perts = perts.melt(
            ignore_index=False, var_name="ENSEMBL", value_name=dpsi_type
        ).dropna().reset_index().copy()
        
        # add gene symbols
        perts = pd.merge(perts, regulators, on="ENSEMBL", how="left")
        
        # format
        perts["regulator"] = perts["ENSEMBL"]
        perts["target"] = perts["EVENT"]
        perts["likelihood"] = np.abs(perts[dpsi_type])
        perts["tfmode"] = (-1)*np.sign(perts[dpsi_type]) # they come from KD or KO, decrease activity
        
        # save
        perts.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule regulon_inference_diff_genexpr:
    input:
        regulators = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors-ensembl.txt"),
        targets = os.path.join(PREP_DIR,"{omic}","{dataset}-EX.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","diff_genexpr","{omic}","{dataset}","regulons.tsv.gz")
    params:
        dpsi_type = "delta_psi-EX"
    threads: 12
    run:
        import pandas as pd
        import numpy as np
        from joblib import Parallel, delayed
        from tqdm import tqdm
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = pd.read_table(input.regulators_oi, header=None)[0].tolist()
        targets = pd.read_table(input.targets, index_col=0)
        dpsi_type = params.dpsi_type
        n_jobs = threads
        
        # subset
        regulators = regulators.loc[regulators.index.isin(regulators_oi)].copy()
        regulators = regulators.loc[~regulators.isnull().all(axis=1)].copy()
        
        common_samples = set(regulators.columns).intersection(targets.columns)
        regulators = regulators[common_samples].copy()
        targets = targets[common_samples].copy()
        
        # make regulons
        def make_regulon(regulator_genexpr, targets, n_samples=5):
            # get top n_samples of regulator genexpr
            x = regulator_genexpr.sort_values()
            samples_low = x.head(n_samples).index
            samples_high = x.tail(n_samples).index
            
            # compute delta PSI (simulate over expression)
            median_genexpr_high = targets[samples_high].median(axis=1)
            median_genexpr_low = targets[samples_low].median(axis=1)
            delta_psi = median_genexpr_high - median_genexpr_low
            
            # prep regulon
            regulon = pd.DataFrame({
                "ENSEMBL": regulator_genexpr.name,
                "EVENT": delta_psi.index,
                dpsi_type: delta_psi.values,
                "median_genexpr_low": median_genexpr_low,
                "median_genexpr_high": median_genexpr_high,
                "n_obs_low": len(samples_low),
                "n_obs_high": len(samples_high),
                "n_missing_low": targets[samples_low].isnull().sum(axis=1),
                "n_missing_high": targets[samples_high].isnull().sum(axis=1)
            })
            regulon["regulator"] = regulon["ENSEMBL"]
            regulon["target"] = regulon["EVENT"]
            regulon["likelihood"] = np.abs(regulon[dpsi_type])
            regulon["tfmode"] = np.sign(regulon[dpsi_type])
            
            regulon = regulon.dropna().copy()
            
            return regulon
        
        regulons = Parallel(n_jobs=n_jobs)(
            delayed(make_regulon)(
                regulators.loc[regulator],
                targets
            )
            for regulator in tqdm(regulators.index)
        )
        regulons = pd.concat(regulons)
        
        # prune
        regulons = regulons.loc[regulons["likelihood"] >= 15]
        
        # save
        regulons.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule regulon_inference_diff_mutation:
    input:
        regulators = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors-ensembl.txt"),
        targets = os.path.join(PREP_DIR,"{omic}","{dataset}-EX.tsv.gz"),
        mutations = os.path.join(PREP_DIR,"mutations","{dataset}.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","regulons","diff_mutation","{omic}","{dataset}","regulons.tsv.gz")
    params:
        dpsi_type = "delta_psi-EX"
    threads: 12
    run:
        import pandas as pd
        import numpy as np
        from joblib import Parallel, delayed
        from tqdm import tqdm
        import gc
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = pd.read_table(input.regulators_oi, header=None)[0].tolist()
        targets = pd.read_table(input.targets, index_col=0)
        mutations = pd.read_table(input.mutations, low_memory=False)
        dpsi_type = params.dpsi_type
        n_jobs = threads
        
        gc.collect()

        # subset
        ## samples
        common_samples = set(regulators.columns).intersection(targets.columns)
        regulators = regulators[common_samples].copy()
        targets = targets[common_samples].copy()
        mutations = mutations.loc[mutations["sampleID"].isin(common_samples)].copy()
        
        ## regulators
        regulators = regulators.loc[~regulators.isnull().all(axis=1)].copy()
        common_regulators = set(
            regulators_oi
        ).intersection(regulators.index).intersection(mutations["ENSEMBL"])
        regulators = regulators.loc[regulators.index.isin(common_regulators)].copy()
        mutations = mutations.loc[mutations["ENSEMBL"].isin(common_regulators)].copy()

        ## consider only regulators with some aberrant mutations
        regulators_mutated = mutations.loc[
            mutations["is_aberrant"],"ENSEMBL"
        ].unique()
        regulators = regulators.loc[regulators_mutated].copy()
        mutations = mutations.loc[mutations["ENSEMBL"].isin(regulators_mutated)].copy()
        
        # make regulons
        def make_regulon(regulator_genexpr, targets, mutations):
            # get mutated samples
            idx_regulator_mutated = (mutations["ENSEMBL"]==regulator_genexpr.name)
            samples_mutated = mutations.loc[
                 idx_regulator_mutated & mutations["is_aberrant"], 
                "sampleID"
            ].unique()
            
            # get wildtype samples (those without any mutation on the regulator)
            samples_wildtype = set(regulator_genexpr.index) - set(mutations.loc[idx_regulator_mutated, "sampleID"])
            
            # compute delta PSI (simulate increase activity)
            delta_psi = targets[samples_wildtype].median(axis=1) - targets[samples_mutated].median(axis=1)
            
            # prep regulon
            regulon = pd.DataFrame({
                "ENSEMBL": regulator_genexpr.name,
                "EVENT": delta_psi.index,
                dpsi_type: delta_psi.values,
                "n_obs_mutated": len(samples_mutated),
                "n_obs_wildtype": len(samples_wildtype),
                "n_missing_mutated": targets[samples_mutated].isnull().sum(axis=1),
                "n_missing_wildtype": targets[samples_wildtype].isnull().sum(axis=1)
            })
            regulon["regulator"] = regulon["ENSEMBL"]
            regulon["target"] = regulon["EVENT"]
            regulon["likelihood"] = np.abs(regulon[dpsi_type])
            regulon["tfmode"] = np.sign(regulon[dpsi_type])
            
            regulon = regulon.dropna().copy()
            
            return regulon
        
        regulons = Parallel(n_jobs=n_jobs)(
            delayed(make_regulon)(
                regulators.loc[regulator],
                targets,
                mutations
            )
            for regulator in tqdm(regulators.index)
        )
        regulons = pd.concat(regulons)
        
        # prune
        regulons = regulons.loc[regulons["likelihood"] >= 15]
        
        # save
        regulons.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        

rule regulon_inference_multivariate_regression:
    input:
        regulators = os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors-ensembl.txt"),
        targets = lambda wildcards: TARGETS_FILES[wildcards.omic] if wildcards.omic!="event_psi" else TARGETS_FILES[wildcards.omic+"_imputed"],
    output:
        os.path.join(RESULTS_DIR,"files","regulons","multivariate_regression","{omic}","{dataset}","regulons.tsv.gz")
    run:
        import pandas as pd
        import numpy as np
        from joblib import Parallel, delayed
        from tqdm import tqdm
        import gc
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = pd.read_table(input.regulators_oi, header=None)[0].tolist()
        targets = pd.read_table(input.targets, index_col=0)
        dpsi_type = params.dpsi_type
        n_jobs = threads
        
        gc.collect()
        
        # load perturbation data
        regulators = pd.read_table(os.path.join(PREP_DIR,'genexpr_tpm','ipsc_differentiation.tsv.gz'), index_col=0)
        targets = pd.read_table(os.path.join(PREP_DIR,'event_psi','ipsc_differentiation-EX.tsv.gz'), index_col=0)
        
        # subset
        regulators = regulators.loc[regulators.index.isin(regulators_oi)].copy()
        
        common_samples = set(regulators.columns).intersection(targets.columns)
        regulators = regulators[common_samples].copy()
        targets = targets[common_samples].copy()
        targets.values[targets==100] = np.nan # this improves sign and worsens identification
        targets.values[targets==0] = np.nan
        targets = targets.loc[targets.notna().sum(axis=1)>20].copy() # improves true positives
        
        # load ground truth
        kd = pd.read_table(os.path.join(PREP_DIR,"ground_truth_pert","ENCOREKD","HepG2","log2_fold_change_tpm.tsv.gz"), index_col=0)
        regulons = pd.read_table(os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons_selected","ENCOREKD_HepG2-dpsi_morethan_15.tsv.gz"))
        
        # how do targets look like for our gene of interest?
        gene_oi_symbol = "U2AF2"
        gene_oi_ensembl = "ENSG00000063244"
        #targets = regulons.loc[regulons["GENE"]==gene_oi_symbol]
        
        corrs = targets.iloc[:20000].corrwith(regulators.loc[gene_oi_ensembl], method="spearman", axis=1)
        corrs.hist(bins=25)
        
        import seaborn as sns
        X = corrs.reset_index().rename(columns={0:"correlation"})
        X["is_target"] = X["EVENT"].isin(regulons["target"])
        X["abs_correlation"] = np.abs(X["correlation"])
        X = pd.merge(X, regulons[["EVENT","delta_psi-EX"]], on="EVENT", how="left")
        X["pos_corr"] = X["correlation"]>0
        X["pos_dpsi"] = -X["delta_psi-EX"]>0
        sns.violinplot(data=X, x="is_target", y="abs_correlation", cut=0)
        sns.scatterplot(data=X, x="correlation", y="delta_psi-EX", alpha=0.5)
        
        x = X.dropna().groupby(["pos_corr","pos_dpsi"]).size().reset_index()
        sns.barplot(data=x, x="pos_dpsi", y=0, hue="pos_corr")
        
        # are regulators perturbed similarly across cancer samples compared with a KD?
        regulators.loc[gene_oi_ensembl].hist(bins=25)
        kd.loc[gene_oi_ensembl,gene_oi_ensembl]
        
        # how much does the expression of other regulators change with changes in gene of interest?
        corrs_regulators = regulators.dropna().T.corr(method="pearson")
        ## get top n_samples of regulator genexpr
        n_samples=25
        x = regulators.loc[gene_oi_ensembl].sort_values()
        samples_low = x.head(n_samples).index
        samples_high = x.tail(n_samples).index
        ## compute log FC (simulate over expression)
        median_genexpr_high = regulators[samples_high].median(axis=1)
        median_genexpr_low = regulators[samples_low].median(axis=1)
        kd_simulated = median_genexpr_low - median_genexpr_high

        X = corrs_regulators[gene_oi_ensembl].reset_index().rename(columns={gene_oi_ensembl:"correlation"})
        X = pd.merge(
            X, 
            kd.loc[kd.index.isin(regulators_oi),gene_oi_ensembl].reset_index().rename(columns={gene_oi_ensembl:"kd_fc"}), 
            on="ID", how="inner"
        )
        X = pd.merge(
            X, 
            kd_simulated.loc[kd_simulated.index.isin(regulators_oi)].reset_index().rename(columns={0:"kd_sim_fc"}), 
            on="ID", how="inner"
        )

        sns.jointplot(data=X, x="kd_fc", y="correlation", kind="reg")
        X["kd_fc"].corr(X["correlation"], method="spearman")
        X["kd_fc"].corr(X["correlation"], method="pearson")

        g = sns.jointplot(data=X, x="kd_fc", y="kd_sim_fc", kind="reg")
        g.plot_joint(sns.kdeplot, color="r", zorder=0, levels=6)
        X["kd_fc"].corr(X["kd_sim_fc"], method="spearman")
        X["kd_fc"].corr(X["kd_sim_fc"], method="pearson")
        
        # SFs change coordinately but not in the same way as when we knockdown a splicing factor
        
        # is the correlation coefficient equivalent to a multivariate regression?
        import matplotlib.pyplot as plt
        import statsmodels.api as sm        

        regulons = regulons.loc[regulons["EVENT"].isin(targets.index)]
        
        for event_oi in regulons.loc[(regulons["GENE"]==gene_oi_symbol),"EVENT"].values[20:30]:
            X = regulators.dropna().T
            y = targets.loc[event_oi]

            is_missing = X.isnull().any(axis=1) | y.isnull()
            X = X.loc[~is_missing]
            y = y[~is_missing]

            X = X - X.median(axis=0).values.reshape(1,-1)

            X["intercept"] = 1

            # Fit the linear regression model
            model_full = sm.OLS(y, X).fit()
            idx = X.columns==gene_oi_ensembl
            model_null = sm.OLS(y, X.loc[:,~idx]).fit()
            lr_stat, lr_pvalue, lr_df_diff = model_full.compare_lr_test(model_null)
            print(lr_pvalue)

            sns.scatterplot(x=X[gene_oi_ensembl], y=y)
            plt.show()
            print("LM:", model_full.params[gene_oi_ensembl])
            print("Spearman:", X[gene_oi_ensembl].corr(y, method="spearman"))
            print(regulons.loc[(regulons["GENE"]==gene_oi_symbol) & (regulons["EVENT"]==event_oi)].T)
        
        print("Done!")

        
rule regulon_inference_arache_java_prep_inputs:
    input:
        regulators = lambda wildcards: os.path.join(PREP_DIR,"genexpr_tpm","{dataset}.tsv.gz") if wildcards.omic!="discretized_qep" else os.path.join(PREP_DIR,"genexpr_tpm_discretized_qep","{dataset}.tsv.gz"),
        regulators_oi = os.path.join(SUPPORT_DIR,"splicing_factors","splicing_factors-ensembl.txt"),
        targets = lambda wildcards: TARGETS_FILES[wildcards.omic] if wildcards.omic!="event_psi" else TARGETS_FILES[wildcards.omic+"_imputed"],
    output:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    threads: 1
    resources:
        runtime = 3600*1, # h
        memory = 5,
    run:
        import pandas as pd
        
        # load
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = list(pd.read_table(input.regulators_oi, header=None)[0])
        targets = pd.read_table(input.targets, index_col=0)
        
        # indices
        regulators.index.name = "regulator"
        targets.index.name = "target"
        
        # subset
        common_regulators = set(regulators_oi).intersection(regulators.index)
        regulators = regulators.loc[common_regulators].copy()
        
        # check order
        common_samples = set(targets.columns).intersection(regulators.columns)
        targets = targets[common_samples].copy()
        regulators = regulators[common_samples].copy()

        # drop events and genes with no variation
        targets = targets.loc[targets.std(1) > 0]
        regulators = regulators.loc[regulators.std(1) > 0]
        
        # save
        regulators.reset_index().to_csv(output.regulators, sep="\t", index=None)
        pd.DataFrame(regulators.index).to_csv(
            output.regulators_oi, sep="\t", index=None, header=False
        )
        targets.reset_index().to_csv(output.targets, sep="\t", index=None)
        
        print("Done!")
        

rule regulon_inference_aracne_java_threshold:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    output:
        touch(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold")),
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        random_seed = 1,
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        mi_pvalue_thresh = "1E-8"
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 5,
    shell:
        """
        set -eo pipefail
        
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --expfile_upstream {input.regulators} \
                --tfs {input.regulators_oi} \
                --expfile_downstream {input.targets} \
                --output {params.output_dir} \
                --pvalue {params.mi_pvalue_thresh} \
                --seed {params.random_seed} \
                --threads {threads} \
                --calculateThreshold
        """
        
        
rule regulon_inference_aracne_java_bootstrap:
    input:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"),
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")
    output:
        touch(os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}"))
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        random_seed = "{boot_i}"
    threads: 6
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 5,
    shell:
        """
        set -eo pipefail

        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --expfile_upstream {input.regulators} \
                --tfs {input.regulators_oi} \
                --expfile_downstream {input.targets} \
                --output {params.output_dir} \
                --pvalue 1E-8 \
                --seed {params.random_seed} \
                --threads {threads}
        """
        
        
rule regulon_inference_aracne_java_consolidation:
    input:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_threshold"),
        [os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}",".done_bootstrap_{boot_i}").format(boot_i=boot_i, omic="{omic}", dataset="{dataset}") for boot_i in BOOTSTRAPS]
    output:
        os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt")
    params:
        aracne_bin = "~/repositories/ARACNe-AP/dist/aracne.jar",
        output_dir = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}"),
        consolidate_pvalue = 0.05
    threads: 12
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 15,
    shell:
        """
        set -eo pipefail
        
        java -Xmx{resources.memory}G -jar {params.aracne_bin} \
                --output {params.output_dir} \
                --threads {threads} \
                --consolidatepvalue {params.consolidate_pvalue} \
                --consolidate
        """
        
        
rule regulon_inference_aracne_spearman_correlation:
    input:
        regulators = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regs.tsv"),
        regulators_oi = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","upstream_regulators.txt"),
        targets = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","targets.tsv")        
    output:
        os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz")
    threads: 12
    run:
        from scipy import stats
        from statsmodels.stats import multitest
        import numpy as np
        import pandas as pd
        from joblib import Parallel, delayed
        from tqdm import tqdm

        def corr(a, b, method):
            idx = np.isfinite(a) & np.isfinite(b)
            a = a[idx]
            b = b[idx]

            corr_funcs = {
                "correlation_spearman": stats.spearmanr,
                "correlation_pearson": stats.pearsonr,
            }
            corr_func = corr_funcs[method]

            try:
                statistic, pvalue = corr_func(a, b)
                result = pd.Series(
                    {
                        "statistic": statistic,
                        "pvalue": pvalue,
                        "n_samples": len(a),
                        "method": method,
                    }
                )
            except:
                result = pd.Series(
                    {
                        "statistic": np.nan,
                        "pvalue": np.nan,
                        "n_samples": np.nan,
                        "method": method,
                    }
                )

            return result


        def compute_correlation_single(targets, upstream_reg_single, method):
            correl = targets.apply(
                lambda x: corr(x, upstream_reg_single, method), axis=1
            ).dropna()
            correl["regulator"] = upstream_reg_single.name
            correl = correl.reset_index()

            # prepare regulon
            ## likelihood
            correl["likelihood"] = np.abs(correl["statistic"])
            ## tfmode
            correl["tfmode"] = correl["statistic"]
            ## keep only significant correlations
            correl["padj"] = np.nan
            _, fdr, _, _ = multitest.multipletests(
                correl.loc[np.isfinite(correl["pvalue"]), "pvalue"], method="fdr_bh"
            )
            correl.loc[np.isfinite(correl["pvalue"]), "padj"] = fdr
            ## filter out irrelevant associations
            #correl = correl.loc[correl["padj"] < THRESH_FDR].copy()

            return correl


        def compute_correlations(targets, regulators, n_jobs, method):
            correls = Parallel(n_jobs=n_jobs)(
                delayed(compute_correlation_single)(targets, regulators.loc[reg_oi], method)
                for reg_oi in tqdm(regulators.index)
            )
            result = pd.concat(correls)
            result = result.reset_index()

            return result        
        
        # load data
        ## load
        targets = pd.read_table(input.targets, index_col=0)
        regulators = pd.read_table(input.regulators, index_col=0)
        regulators_oi = list(pd.read_table(input.regulators_oi, header=None)[0])
        n_jobs = threads
        ## index
        targets.index.name = "target"
        regulators.index.name = "regulator"
        ## subset
        genes_oi = set(regulators_oi).intersection(regulators.index)
        regulators = regulators.loc[genes_oi].copy()
        ## check order
        common_samples = set(targets.columns).intersection(regulators.columns)
        targets = targets[common_samples].copy()
        regulators = regulators[common_samples].copy()        
        
        print(targets.shape, regulators.shape)
        
        # compute correlations
        result = compute_correlations(
            targets, regulators, n_jobs, "correlation_spearman"
        )
        
        # save
        print("Saving data...")
        result.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
    
rule regulon_inference_aracne_java_prep_output:
    input:
        regulons = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","network.txt"),
        spearman = os.path.join(RESULTS_DIR,"files","regulons","correlation_spearman","{omic}","{dataset}.tsv.gz")
    output:
        regulons = os.path.join(RESULTS_DIR,"files","regulons","aracne","{omic}","{dataset}","regulons.tsv.gz")
    resources:
        runtime = 3600*12, # h (event PSI needs more time)
        memory = 15,
    run:
        import pandas as pd
        
        # load
        regulons = pd.read_table(input.regulons)
        spearman = pd.read_table(input.spearman)
        
        # prepare regulon
        ## "regulator" and "target" columns
        regulons["regulator"] = regulons["Regulator"]
        regulons["target"] = regulons["Target"]
        
        ## likelihood
        regulons["likelihood"] = regulons["MI"]
        
        ## tfmode
        regulons = pd.merge(
            regulons,
            spearman[["regulator", "target", "tfmode"]],
            on=["regulator", "target"],
            how="left",
        )
        
        regulons.loc[regulons["tfmode"].isnull(), "tfmode"] = 0
        
        # save
        regulons.reset_index().to_csv(output.regulons, sep="\t", index=None, compression="gzip")

        print("Done!")
        

rule subset_regulons_lessthan:
    input:
        regulons = lambda wildcards: REGULONS[wildcards.dataset]
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_lessthan_{thresh}.tsv.gz")
    params:
        thresh = "{thresh}"
    run:
        import pandas as pd
        
        regulons = pd.read_table(input.regulons)
        thresh = float(params.thresh)
        
        regulons = regulons.loc[regulons["likelihood"] <= thresh]
        
        regulons.to_csv(output.regulons, **SAVE_PARAMS)
        
        print("Done!")
        
        
rule subset_regulons_morethan:
    input:
        regulons = lambda wildcards: REGULONS[wildcards.dataset]
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_{thresh}.tsv.gz")
    params:
        thresh = "{thresh}"
    run:
        import pandas as pd
        
        regulons = pd.read_table(input.regulons)
        thresh = float(params.thresh)
        
        regulons = regulons.loc[regulons["likelihood"] >= thresh]
        
        regulons.to_csv(output.regulons, **SAVE_PARAMS)
        
        print("Done!")

        
rule compute_protein_activity_splicing_pert_rnaseq:
    input:
        signature = os.path.join(PREP_DIR,"ground_truth_pert","{dataset_signature}","{cell_line}","delta_psi-EX.tsv.gz"),
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset_regulon}","dpsi_{thresh_type}_{thresh}.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz")
    params:
        assoc_method = "ground_truth",
        actinf_method = "viper"
    shell:
        """
        Rscript scripts/infer_protein_activity.R \
                    --signature_file={input.signature} \
                    --regulons_file={input.regulons} \
                    --output_file={output} \
                    --assoc_method={params.assoc_method} \
                    --actinf_method={params.actinf_method}
        """
        
        
rule combine_inferred_protein_activities_splicing_pert_rnaseq:
    input:
        protein_activities = [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line, thresh_type="lessthan", thresh=thresh) for
            dataset_regulon in REGULONS.keys() for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES for thresh in THRESHS_DPSI["lessthan"]
        ] + [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{thresh_type}_{thresh}.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line, thresh_type="morethan", thresh=thresh) for
            dataset_regulon in REGULONS.keys() for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES for thresh in THRESHS_DPSI["morethan"]
        ]
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-dpsi.tsv.gz")
    run:
        import os
        import pandas as pd
        
        dfs = []
        for file in input.protein_activities:
            dataset_regulon = os.path.basename(os.path.dirname(file))
            dataset_signature = os.path.basename(file).split("-")[0]
            cell_line = os.path.basename(file).split("-")[1]
            thresh_type = os.path.basename(file).split("-")[2].replace(".tsv.gz","").split("_")[1]
            thresh = os.path.basename(file).split("-")[2].replace(".tsv.gz","").split("_")[2]
            
            df = pd.read_table(file)
            df = df.melt(id_vars="regulator", var_name="PERT_GENE", value_name="protein_activity")
            
            df["dataset_regulon"] = dataset_regulon
            df["dataset_signature"] = dataset_signature
            df["cell_line"] = cell_line
            df["thresh_type"] = thresh_type
            df["thresh"] = float(thresh)
            
            dfs.append(df)
            
        dfs = pd.concat(dfs)
        
        dfs.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule compute_protein_activity_splicing_delta_psi:
    input:
        signature = os.path.join(PREP_DIR,"ground_truth_pert","{dataset_signature}","{cell_line}","delta_psi-EX.tsv.gz"),
        regulons = os.path.join(RESULTS_DIR,"files","regulons","{diff_type}","event_psi","{dataset_regulon}","regulons.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{diff_type}.tsv.gz")
    params:
        assoc_method = "ground_truth",
        actinf_method = "viper"
    shell:
        """
        Rscript scripts/infer_protein_activity.R \
                    --signature_file={input.signature} \
                    --regulons_file={input.regulons} \
                    --output_file={output} \
                    --assoc_method={params.assoc_method} \
                    --actinf_method={params.actinf_method}
        """
        
        
rule combine_inferred_protein_activities_delta_psi:
    input:
        protein_activities = [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-dpsi_{diff_type}.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line, diff_type="{diff_type}") for
            dataset_regulon in ARACNE_DATASETS for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES
        ]
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-dpsi_{diff_type}.tsv.gz")
    run:
        import os
        import pandas as pd
        import numpy as np
        
        dfs = []
        for file in input.protein_activities:
            dataset_regulon = os.path.basename(os.path.dirname(file))
            dataset_signature = os.path.basename(file).split("-")[0]
            cell_line = os.path.basename(file).split("-")[1]
            thresh_type = "morethan"
            thresh = 15
            
            df = pd.read_table(file)
            df = df.melt(id_vars="regulator", var_name="PERT_GENE", value_name="protein_activity")
            
            df["dataset_regulon"] = dataset_regulon
            df["dataset_signature"] = dataset_signature
            df["cell_line"] = cell_line
            df["thresh_type"] = thresh_type
            df["thresh"] = thresh
            
            dfs.append(df)
            
        dfs = pd.concat(dfs)
        
        dfs.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")
        
        
rule compute_protein_activity_genexpr:
    input:
        signature = os.path.join(PREP_DIR,"ground_truth_pert","{dataset_signature}","{cell_line}","log2_fold_change_tpm.tsv.gz"),
        regulons = os.path.join(RESULTS_DIR,"files","regulons","aracne","genexpr_tpm","{dataset_regulon}","regulons.tsv.gz")
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-fc_tpm.tsv.gz")
    params:
        assoc_method = "ground_truth",
        actinf_method = "viper"
    shell:
        """
        Rscript scripts/infer_protein_activity.R \
                    --signature_file={input.signature} \
                    --regulons_file={input.regulons} \
                    --output_file={output} \
                    --assoc_method={params.assoc_method} \
                    --actinf_method={params.actinf_method}
        """
        

rule combine_inferred_protein_activities_genexpr:
    input:
        protein_activities = [
            os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","{dataset_regulon}","{dataset_signature}-{cell_line}-fc_tpm.tsv.gz").format(dataset_regulon=dataset_regulon, dataset_signature=dataset_signature, cell_line=cell_line) for
            dataset_regulon in ARACNE_DATASETS for dataset_signature in ENCORE_DATASETS for cell_line in CELL_LINES
        ]
    output:
        os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-fc_tpm.tsv.gz")
    run:
        import os
        import pandas as pd
        import numpy as np
        
        dfs = []
        for file in input.protein_activities:
            dataset_regulon = os.path.basename(os.path.dirname(file))
            dataset_signature = os.path.basename(file).split("-")[0]
            cell_line = os.path.basename(file).split("-")[1]
            thresh_type = "aracne"
            thresh = "aracne"
            
            df = pd.read_table(file)
            df = df.melt(id_vars="regulator", var_name="PERT_GENE", value_name="protein_activity")
            
            df["dataset_regulon"] = dataset_regulon
            df["dataset_signature"] = dataset_signature
            df["cell_line"] = cell_line
            df["thresh_type"] = thresh_type
            df["thresh"] = thresh
            
            dfs.append(df)
            
        dfs = pd.concat(dfs)
        
        dfs.to_csv(output[0], **SAVE_PARAMS)
        
        print("Done!")

        
rule evaluate_regulons:
    input:
        protein_activity = os.path.join(RESULTS_DIR,"files","subsetted_regulons","protein_activity","merged-{data_type}.tsv.gz")
    output:
        evaluation_rankings = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings-{data_type}.tsv.gz"),
        evaluation_corrs = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs-{data_type}.tsv.gz")
    run:
        import pandas as pd
        from scipy import stats
        
        df = pd.read_table(input.protein_activity)
        
        vars_oi = ["thresh_type","thresh","dataset_regulon","dataset_signature","cell_line"]
        
        # ranking of perturbations across experiments
        df["ranking_between"] = df.groupby(vars_oi + ["regulator"])["protein_activity"].rank()
        df["rankperc_between"] = df["ranking_between"] / df.groupby(vars_oi + ["regulator"])["ranking_between"].transform('max')
        # ranking of perturbations within experiments
        df["ranking_within"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].rank()
        df["rankperc_within"] = df["ranking_within"] / df.groupby(vars_oi + ["PERT_GENE"])["ranking_within"].transform('max')
        df["eval_type"] = "real"
        # correlation between protein activities in both cell lines and regulons
        corrs = []
        for regulon in df["dataset_regulon"].unique():
            idx = (df["dataset_regulon"]==regulon) & (df["regulator"]==df["PERT_GENE"])
            X = df.loc[idx].pivot(
                index=["thresh_type","thresh","dataset_regulon","dataset_signature","regulator","PERT_GENE"],
                columns="cell_line",
                values="protein_activity"
            )
            pearson_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="pearson").iloc[0::2,-1]
            pearson_coef.name = "pearson_coef"
            spearman_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="spearman").iloc[0::2,-1]
            spearman_coef.name = "spearman_coef"
            corrs.append(
                pd.concat([pearson_coef, spearman_coef], axis=1).reset_index()
            )
        corrs = pd.concat(corrs)
        corrs["cell_line"] = "HepG2_vs_K562"
        corrs["eval_type"] = "real"
        
        # store
        evaluation_rankings = df.copy()
        evaluation_corrs = corrs.copy()

        # randomize
        df["protein_activity"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].sample(frac=1, random_state=1).values
        # ranking of perturbation across experiments
        df["ranking_between"] = df.groupby(vars_oi + ["regulator"])["protein_activity"].rank()
        df["rankperc_between"] = df["ranking_between"] / df.groupby(vars_oi + ["regulator"])["ranking_between"].transform('max')
        # ranking of perturbation within experiments
        df["ranking_within"] = df.groupby(vars_oi + ["PERT_GENE"])["protein_activity"].rank()
        df["rankperc_within"] = df["ranking_within"] / df.groupby(vars_oi + ["PERT_GENE"])["ranking_within"].transform('max')
        # add ranking type
        df["eval_type"] = "random"
        # correlation between protein activities in both cell lines and regulons
        corrs = []
        for regulon in df["dataset_regulon"].unique():
            idx = (df["dataset_regulon"]==regulon) & (df["regulator"]==df["PERT_GENE"])
            X = df.loc[idx].pivot(
                index=["thresh_type","thresh","dataset_regulon","dataset_signature","regulator","PERT_GENE"],
                columns="cell_line",
                values="protein_activity"
            )
            pearson_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="pearson").iloc[0::2,-1]
            pearson_coef.name = "pearson_coef"
            spearman_coef = X.reset_index().groupby(["thresh_type","thresh","dataset_regulon","dataset_signature"])[["HepG2","K562"]].corr(method="spearman").iloc[0::2,-1]
            spearman_coef.name = "spearman_coef"
            corrs.append(
                pd.concat([pearson_coef, spearman_coef], axis=1).reset_index()
            )
        corrs = pd.concat(corrs)
        corrs["cell_line"] = "HepG2_vs_K562"
        corrs["eval_type"] = "random"
        
        # store
        evaluation_rankings = pd.concat([evaluation_rankings, df])
        evaluation_corrs = pd.concat([evaluation_corrs, corrs])
        
        # save
        evaluation_rankings.to_csv(output.evaluation_rankings, **SAVE_PARAMS)
        evaluation_corrs.to_csv(output.evaluation_corrs, **SAVE_PARAMS)
        
        print("Done!")

        
rule selected_regulons:
    input:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons","{dataset}","dpsi_morethan_15.tsv.gz")
    output:
        regulons = os.path.join(RESULTS_DIR,"files","subsetted_regulons","regulons_selected","{dataset}-dpsi_morethan_15.tsv.gz")
    shell:
        """
        set -eo pipefail
        
        cp {input.regulons} {output.regulons}
        
        echo "Done!"
        """


rule figures_validation_activity_encore:
    input:
        evaluation_rankings = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_rankings-{omic_type}.tsv.gz"),
        evaluation_corrs = os.path.join(RESULTS_DIR,"files","subsetted_regulons","evaluation_corrs-{omic_type}.tsv.gz")
    output:
        directory(os.path.join(RESULTS_DIR,'figures','evaluation-{omic_type}'))
    params:
        omic_type = "{omic_type}"
    shell:
        """
        Rscript scripts/figures_evaluation_activity_encore.R \
                    --evaluation_rankings_file={input.evaluation_rankings} \
                    --evaluation_corrs_file={input.evaluation_corrs} \
                    --omic_type={params.omic_type} \
                    --figs_dir={output}
        """